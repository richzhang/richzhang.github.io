
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252" charset="utf-8">
        <title>Richard Zhang - Research Scientist, Adobe Research </title>
<style type="text/css"></style></head>
    <body><table border="0" width="980px" align="center"><tbody><tr><td>
    
        </td><td valign="top">
<!--            <img src="images/cmuscslogo.gif">
                <img src="images/rilogo.png"> -->
        <br>
        <table style="font-size: 11pt;" border="0" width="100%">
            <tbody><tr>
                <td width="50%">
                    <!-- <img width="300" src="./index_files/mypic.jpeg" border="0"> -->
                    <!-- <img width="300" src="./index_files/mypic2.jpg" border="0"> -->
                    <img width="250" src="./index_files/mypic3.jpg" border="0">
                </td>
                <td>
                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="6"> 
                        <b>Richard Zhang</b><br><br>
                    </font>
                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="4"> 
                        Senior Research Scientist<br>
                        Adobe Research<br>
                        San Francisco, CA<br><br>
                        rizhang at adobe.com<br>
                        [<a href="https://github.com/richzhang" border="0">GitHub</a>]
                        [<a href="https://scholar.google.com/citations?user=LW8ze_UAAAAJ&hl=en" border="0">Google Scholar</a>]<br>
                        [<a href="index_files/CV.pdf" border="0">Resume/CV</a>]
                        [<a href="https://twitter.com/rzhang88" border="0">Twitter</a>]
                        [<a href="bio.txt" border="0">Bio</a>]<br>
                    </font>
                </td>
            </tr>
        </tbody></table> 
        <p>
        </p><hr size="2" align="left" noshade="">
        <p>
        
        <font face="helvetica, ariel, &#39;sans serif&#39;">
        <!--</font></p><h2><font face="helvetica, ariel, &#39;sans serif&#39;">About Me</font></h2><font face="helvetica, ariel, &#39;sans serif&#39;">-->
        My research interests are in computer vision, machine learning, deep learning, graphics, and image processing. I obtained a PhD at UC Berkeley, advised by Prof. <a href="http://www.eecs.berkeley.edu/~efros/">Alexei (Alyosha) Efros</a>. I obtained BS and MEng degrees from Cornell University in ECE. I often collaborate with academic researchers, either through internships or university collaboration.

        <!-- <br><br> -->
        </p><hr size="2" align="left" noshade="">

        <h3>News </h3>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <span style="font-size: 10pt;">
            <b>[Oct 2022]</b> I am presenting some posters at ECCV. BlobGAN on Tue, Oct 25, 1100-1330, 3D-FM GAN at 1530-1730, and Anyres-GAN on Wed, Oct 26, 1100-1330. See you there!<br>
            <b>[Oct 2022]</b> I am co-organizing the <a href="https://she-workshop.github.io/">Sketching for Human Expressivity Workshop</a> at ECCV on Sun, Oct 23rd. See you there!<br>
            <b>[Sept 2022]</b> 爷爷好👋<br>
            <b>[Jul 2022]</b> Any-Resolution GANs, BlobGAN, and 3D-FM GAN were accepted to ECCV 2022.<br>
            <b>[Jun 2022]</b> Our GANgealing work was one of the 33 best paper finalists at CVPR 2022.<br>
            <b>[Jun 2022]</b> I spoke about "Anycost and Anyres GANs for Image Synthesis" at the <a href="https://data.vision.ee.ethz.ch/cvl/ntire22/">NTIRE</a> and <a  href="https://ai4cc.net/">AICC</a> CVPR 2022 workshops.<br>
            <b>[May 2022]</b> Please see our SIGGRAPH 2022 work, ASSET, which enables high-resolution semantic editing with transformers.<br>
            <b>[Apr 2022]</b> Our work on GANgealing was covered by <a href="https://www.youtube.com/watch?v=qtOkktTNs-k">2-minute papers</a>.<br>
            <b>[Mar 2022]</b> Our works on GANgealing and Vision-Aided GANs were accepted to CVPR as oral presentations.<br>
            <b>[Mar 2022]</b> Our clean-fid work was accepted to CVPR. <mark><font face="Courier">pip install clean-fid</font></mark> to try it out!<br>
            <b>[Oct 2021]</b> See Landscape mixer in Photoshop Neural Filters, based on our Swapping Autoencoder work.<br>
            <!-- <b>[July 2021]</b> Our work on editing NeRFs was accepted to ICCV.<br> -->
            <!-- <b>[Apr 2021]</b> Our work on using generative models to improve discriminative models was accepted to CVPR.<br> -->
            <!-- <b>[Apr 2021]</b> Our work on few-shot GAN training was accepted to CVPR.<br> -->
            <!-- <b>[Mar 2021]</b> Our works on speeding up unconditional GANs for image editing and projection (AnyCost GANs) and conditional GANs with implicit functions (ASAPnet) were accepted to CVPR.<br> -->
            <!-- <b>[Mar 2021]</b> Our work on audio perceptual metrics was accepted to ICASSP.<br> -->
            <!-- <b>[Sept 2020]</b> Few updates regarding <a href="https://richzhang.github.io/antialiased-cnns/">Antialiasing CNNs</a> [ICML 2019], which can <b>stabilize and improve the backbone for your application</b>:<br>
            - Easy installation: <mark><font face="Courier">pip install antialiased-cnns</font></mark> and 
            <mark><font face="Courier" color="red">import</font>
            <font face="Courier">antialiased_cnns; model</font>
            <font face="Courier" color="blue"> = </font>
            <font face="Courier">antialiased_cnns.</font><font face="Courier" color="#6c53b5">resnet50</font><font face="Courier">(pretrained=</font>
            <font face="Courier" color="blue">True)</font></mark><br>
            - For more information, including "What is Aliasing?", see my <a href="https://youtu.be/8CXrplBG-SE?t=1049">guest lecture</a> [15 min] in SFU CMPT 361, Intro to Vision, Sampling and Aliasing lecture.<br>
            - A nice followup work,
            <a href="https://maureenzou.github.io/ddac/">Delving Deeper into Antialiasing in Convnets</a> by Zou, Xiao, Yu, & Lee, won best paper at BMVC 2020. Check it out!<br> -->
            <!-- <b>[Aug 2020]</b> I gave a talk on <a href="https://www.youtube.com/watch?v=CYdYWeTE-CI">Detecting Generated Imagery, Deep and Shallow</a> (35 min) at the <a href="https://sense-human.github.io/">Sensing Humans</a> workshop at ECCV.<br> -->
            <!-- <b>[Aug 2020]</b> I gave a talk on <a href="https://www.youtube.com/watch?v=aM86tOniH90">Style and Structure Disentanglement for Image Manipulation</a> (30 min) at the <a href="https://data.vision.ee.ethz.ch/cvl/aim20/">Advances in Image Manipulation</a> workshop at ECCV.<br> -->
            <!-- <b>[Aug 2020]</b> I gave a talk on <a href="https://www.bilibili.com/video/BV1e7411c7kR?p=46">Analyzing Artifacts in Discriminative and Generative Models</a> (40 min) at the GAMES webinar.<br> -->
            <!-- <b>[July 2020]</b> Our work on using contrastive learning for unpaired translation was accepted to ECCV.<br> -->
            <!-- <b>[July 2020]</b> Our work on inverting GANs was accepted to ECCV as an oral.<br> -->
            <!-- <b>[July 2020]</b> See our new work on Swapping Autoencoders below.<br> -->
            <!-- <b>[July 2020]</b> Our work on audio perceptual metrics was accepted to Intespeech.<br> -->
            <!-- <b>[Feb 2020]</b> I served as an Area Chair for CVPR 2020 and spoke on <a href="https://www.youtube.com/watch?v=aNDwHRxWTa0">Analyzing CNN Artifacts in Discriminative and Generative Models</a> (11 min). The second half includes our "Detecting CNN-generated images" work, just accepted to CVPR.<br> -->
            <!-- <b>[Dec 2019]</b> See our new work on detecting CNN-generated images below.<br> -->
            <!-- <b>[Nov 2019]</b> I presented our "Detecting Photoshop" ICCV19 work at <a href="https://www.youtube.com/watch?v=21lj8tCSMkg">Adobe MAX</a> (5 min), on stage with John Mulaney (aka Peter Porker/Spider-Ham)!<br> -->
            <!-- <b>[Oct 2019]</b> Thank you <a href="https://twitter.com/Oxford_VGG/status/1184087868857290752">Oxford</a> and UCL for hosting me.<br> -->
            <!-- <b>[Oct 2019]</b> This <a href="http://video.tv.adobe.com/v/28291">video</a> shows interactive colorization in Photoshop Elements 2020, based on our SIGGRAPH 2017 work.<br> -->
            <!-- <b>[Sept 2019]</b> See our new work on interactive sketch to image synthesis below.<br> -->
            <!-- <b>[Jun 2019]</b> See our new work on detecting Photoshopped images below.<br> -->
            <!-- <b>[May 2019]</b> Our work on anti-aliasing convolutional networks has been accepted to ICML 2019. Try anti-aliasing your convnet <a href="https://github.com/adobe/antialiased-cnns">here</a>!<br> -->
            <!-- <b>[Aug 2018]</b> I will be presenting at the Thesis Fast Forward session at SIGGRAPH on Tuesday 8/14, 2:00pm.<br> -->
            <!-- <b>[Jun 2018]</b> We will be presenting our <a href="https://richzhang.github.io/PerceptualSimilarity/">project</a> on perceptual metrics at CVPR, Tuesday 6/19, 10:10am. Try our metric <a href="https://github.com/richzhang/PerceptualSimilarity">here</a>!<br> -->
            <!-- <b>[May 2018]</b> I have <a href="./index_files/graduation.jpg">graduated</a> from UC Berkeley and have joined Adobe Research as a Research Scientist in San Francisco! -->
            </span>

<!--         </p><hr size="2" align="left" noshade="">

        <h3>Internship </h3>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <span style="font-size: 10pt;">
            If you have similar interests and are interested in collaborating during a Summer 2023 internship, I'd be happy to hear from you! <b>Please apply <a href="https://research.adobe.com/careers/internships/">here</a> first</b>. Tell me about your past research experience and what you would potentially like to do. The goal of an internship is a publication, usually CVPR or SIGGRAPH. Interns are typically PhD students; the number of slots is limited, so we unfortunately cannot accept everyone.
            </span>
        </font>
 -->
        </p><hr size="2" align="left" noshade="">

        <h2>Publications </h2>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
                <tr>
                    <td width="30%" align=center>
                        <img width="250" align="center" src="index_files/custom_diffusion_teaser.png" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Multi-Concept Customization of Text-to-Image Diffusion</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://nupurkmr9.github.io/">Nupur Kumari</a>, Bingliang Zhang, Richard Zhang, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In ArXiv, 2022. <br>
                        [<a href="https://arxiv.org/abs/2212.04488">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~custom-diffusion/">Webpage</a>]
                        [<a href="https://github.com/adobe-research/custom-diffusion">GitHub</a>]
                        [<a href="https://github.com/adobe-research/custom-diffusion#references">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="100" align="center" src="https://github.com/chail/anyres-gan/blob/main/img/github_loop.gif?raw=true" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Any-resolution Training for High-resolution Image Synthesis</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/lrchai/">Lucy Chai</a>, <a href="http://www.mgharbi.com/">Michaël Gharbi</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, Richard Zhang<br>
                        In ECCV, 2022. <br>
                        [<a href="https://arxiv.org/abs/2204.07156">Paper</a>]
                        [<a href="https://chail.github.io/anyres-gan/">Webpage</a>]
                        [<a href="https://github.com/chail/anyres-gan">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=0l11u2HzQrQ&feature=emb_title&ab_channel=LucyChai">Video</a>]
                        [<a href="https://chail.github.io/anyres-gan/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <!-- <img width="100" align="center" src="https://dave.ml/blobgan/static/vids/moveall_bg.mp4" border="0"> &nbsp; -->
                        <video src="https://dave.ml/blobgan/static/vids/moveall_bg.mp4" data-no-pause="" autoplay="" playsinline="" muted="" loop="" width=180></video>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>BlobGAN: Spatially Disentangled Scene Representations</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://dave.ml/">Dave Epstein</a>, <a href="https://taesung.me/">Taesung Park</a>, Richard Zhang, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a><br>
                        In ECCV, 2022. <br>
                        [<a href="https://arxiv.org/abs/2205.02837">Paper</a>]
                        [<a href="https://dave.ml/blobgan/">Webpage</a>]
                        [<a href="https://github.com/dave-epstein/blobgan">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=KpUv82VsU5k">Video</a>]
                        [<a href="https://dave.ml/blobgan/#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="200" align="center" src="https://lychenyoko.github.io/3D-FM-GAN-Webpage/resources/Teaser.gif?raw=true" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>3D-FM GAN: Towards 3D-Controllable Face Manipulation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://lychenyoko.github.io/">Yuchen Liu</a>,
                        <a href="https://zhixinshu.github.io/">Zhixin Shu</a>,
                        <a href="https://yijunmaverick.github.io/">Yijun Li</a>,
                        <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
                        Richard Zhang,
                        <a href="https://ece.princeton.edu/people/sun-yuan-kung">Sun-Yuan Kung</a> <br>
                        In ECCV, 2022. <br>
                        [<a href="">Paper</a>]
                        [<a href="https://lychenyoko.github.io/3D-FM-GAN-Webpage/">Webpage</a>]
                        <!-- [<a href="">GitHub</a>] -->
                        [<a href="https://www.youtube.com/watch?v=3tR7qIXyzLE">Video</a>]
                        <!-- [<a href="">Bibtex</a>] -->
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="180" align="center" src="https://people.cs.umass.edu/~dliu/df_files/sig22.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://people.cs.umass.edu/~dliu/">Difan Liu</a>, Sandesh Shetty, <a href="http://www.tobiashinz.com/">Tobias Hinz</a>, <a href="https://techmatt.github.io/">Matthew Fisher</a>, Richard Zhang, <a href="https://taesung.me/">Taesung Park</a>, <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a><br>
                        In SIGGRAPH, 2022. <br>
                        [<a href="https://arxiv.org/abs/2205.12231">Paper</a>]
                        [<a href="https://people.cs.umass.edu/~dliu/projects/ASSET/">Webpage</a>]
                        [<a href="https://github.com/DifanLiu/ASSET">GitHub</a>]
                        [<a href="https://people.cs.umass.edu/~dliu/projects/ASSET/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=center>
                        <img width="230" align="center" src="index_files/cats_cube_3.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>GAN-Supervised Dense Visual Alignment</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.wpeebles.com/">William Peebles</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                        Richard Zhang,
                        <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
                        <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a><br>
                        In CVPR (oral, best paper finalist), 2022. <br>
                        [<a href="https://arxiv.org/abs/2112.05143">Paper</a>]
                        [<a href="https://www.wpeebles.com/gangealing.html">Webpage</a>]
                        [<a href="https://github.com/wpeebles/gangealing">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=Qa1ASS_NuzE">Video</a>]
                        [<a href="https://www.wpeebles.com/gangealing_bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="180" align="center" src="index_files/arxiv2021_visionaid_teaser.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Ensembling Off-the-shelf Models for GAN Training</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://nupurkmr9.github.io/">Nupur Kumari</a>,
                        Richard Zhang,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In CVPR (oral), 2022. <br>
                        [<a href="https://arxiv.org/abs/2112.09130">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~vision-aided-gan/">Webpage</a>]
                        [<a href="https://github.com/nupurkmr9/vision-aided-gan">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=oHdyJNdQ9E4">Video</a>]
                        [<a href="index_files/bibtex_arxiv2021_visionaid.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="240" align="center" src="https://gauravparmar.com/images/teaser_prepped_website.jpg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://gauravparmar.com/">Gaurav Parmar</a>,
                        <a href="https://yijunmaverick.github.io/">Yijun Li</a>,
                        <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>,
                        Richard Zhang,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                        <a href="http://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>
                        <br>
                        In CVPR, 2022. <br>
                        [<a href="https://arxiv.org/abs/2206.08357">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~SAMInversion/">Webpage</a>]
                        [<a href="https://github.com/adobe-research/sam_inversion">GitHub</a>]
                        [<a href="https://www.cs.cmu.edu/~SAMInversion/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="https://raw.githubusercontent.com/GaParmar/clean-fid/main/docs/images/resize_circle.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>On Aliased Resizing Libraries and Surprising Subtleties in FID Calculation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://gauravparmar.com/">Gaurav Parmar</a>, Richard Zhang, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In CVPR, 2022. <br>
                        [<a href="https://arxiv.org/abs/2104.11222">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~clean-fid/">Webpage</a>]
                        [<a href="https://github.com/GaParmar/clean-fid">GitHub</a>]
                        [<a href="https://github.com/GaParmar/clean-fid#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="index_files/editnerf.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Editing Conditional Radiance Fields</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/stevenliu/">Steven Liu</a>, <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>, <a href="https://ztzhang.info/">Zhoutong Zhang</a>, Richard Zhang, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="http://bryanrussell.org/">Bryan Russell</a><br>
                        In ICCV, 2021. <br>
                        [<a href="https://arxiv.org/abs/2105.06466">Paper</a>]
                        [<a href="http://editnerf.csail.mit.edu/">Webpage</a>]
                        [<a href="https://github.com/stevliu/editnerf">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=9qwRD4ejOpw">Video</a>]
                        [<a href="https://colab.research.google.com/github/stevliu/editnerf/blob/master/editnerf.ipynb">Demo</a>]
                        [<a href="https://github.com/stevliu/editnerf#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="200" align="center" src="index_files/cfl_teaser.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Contrastive Feature Loss for Image Prediction</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.alexandonian.com/">Alex Andonian</a>,
                        <a href="https://taesung.me/">Taesung Park</a>,
                        <a href="http://bryanrussell.org/">Bryan Russell</a>,
                        <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                        Richard Zhang.<br>
                        In ICCV AIM Workshop, 2021. <br>
                        [<a href="https://arxiv.org/abs/2111.06934">Paper</a>]
                        [<a href="https://github.com/alexandonian/contrastive-feature-loss">GitHub</a>]
                        [<a href="index_files/bibtex_iccvaim21_cfl.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="180" align="center" src="https://github.com/chail/gan-ensembling/blob/main/img/teaser.gif?raw=true" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Ensembling with Deep Generative Views</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/lrchai//">Lucy Chai</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, Richard Zhang<br>
                        In CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2104.14551">Paper</a>]
                        [<a href="https://chail.github.io/gan-ensembling/">Webpage</a>]
                        [<a href="https://github.com/chail/gan-ensembling">GitHub</a>]
                        [<a href="https://www.youtube.com/channel/UCty2ywzQwRx-qpbV1S8EiKQ">Video</a>]
                        [<a href="https://colab.research.google.com/drive/1-qZBjn07KlWv27kKQGaKOXMBgP-Fb0Ws?usp=sharing">Colab</a>]
                        [<a href="https://chail.github.io/gan-ensembling/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="https://github.com/utkarshojha/few-shot-gan-adaptation/blob/gh-pages/resources/concept.gif?raw=true" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Few-shot Image Generation via Cross-domain Correspondence</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://utkarshojha.github.io/">Utkarsh Ojha</a>, <a href="https://yijunmaverick.github.io/">Yijun Li</a>, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>, <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, <a href="https://web.cs.ucdavis.edu/~yjlee/">Yong Jae Lee</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, Richard Zhang<br>
                        In CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2104.06820">Paper</a>]
                        [<a href="https://utkarshojha.github.io/few-shot-gan-adaptation/">Webpage</a>]
                        [<a href="https://github.com/utkarshojha/few-shot-gan-adaptation">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=GCDm8hOlpHs">Video</a>]
                        [<a href="https://utkarshojha.github.io/few-shot-gan-adaptation/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="https://camo.githubusercontent.com/50c7deee28f1b4476c43c32ded041a63bd3a120c5966ba0c1d70fbbff9b68217/68747470733a2f2f68616e6c61622e6d69742e6564752f70726f6a656374732f616e79636f73742d67616e2f696d616765732f666c657869626c652e676966" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Anycost GANs for Interactive Image Synthesis and Editing</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://linji.me/">Ji Lin</a>, Richard Zhang, Frieder Ganz, <a href="https://songhan.mit.edu/">Song Han</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2103.03243">Paper</a>]
                        [<a href="https://hanlab.mit.edu/projects/anycost-gan/">Webpage</a>]
                        [<a href="https://www.youtube.com/watch?v=_yEziPl9AkM">Video</a>]
                        [<a href="https://github.com/mit-han-lab/anycost-gan">GitHub</a>]
                        [<a href="https://github.com/mit-han-lab/anycost-gan#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img height="90" align="center" src="index_files/asapnet_teaser.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Spatially-Adaptive Pixelwise Networks for Fast Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://stamarot.webgr.technion.ac.il/">Tamar Rott Shaham</a>, <a href="http://www.mgharbi.com/">Michaël Gharbi</a>, Richard Zhang, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="https://tomer.net.technion.ac.il/">Tomer Michaeli</a><br>
                        In CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2012.02992">Paper</a>]
                        [<a href="https://tamarott.github.io/ASAPNet_web/">Webpage</a>]
                        [<a href="https://tamarott.github.io/ASAPNet_web/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="220" align="center" src="./index_files/cdpam_teaser.jpg" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>CDPAM: Contrastive Learning for Perceptual Audio Similarity</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.cs.princeton.edu/~pmanocha/">Pranay Manocha</a>, <a href="https://research.adobe.com/person/zeyu-jin/">Zeyu Jin</a>, Richard Zhang, <a href="https://www.cs.princeton.edu/~af/">Adam Finkelstein</a><br>
                        In ICASSP, 2021. <br>
                        [<a href="https://arxiv.org/abs/2102.05109">Paper</a>]
                        [<a href="https://pixl.cs.princeton.edu/pubs/Manocha_2021_CCL/index.php">Webpage</a>]
                        [<a href="https://github.com/pranaymanocha/PerceptualAudio">GitHub</a>]
                        [<a href="./index_files/bibtex_icassp2021_audio.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="230" align="center" src="index_files/arxiv21_expandcollapse_teaser.jpg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>The Low-Rank Simplicity Bias in Deep Networks</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/minhuh/">Minyoung Huh</a>, <a href="https://people.csail.mit.edu/hmobahi/">Hossein Mobahi</a>, Richard Zhang, <a href="https://scholar.google.com/citations?user=7N-ethYAAAAJ&hl=en">Brian Cheung</a>, <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a><br>
                        In ArXiv, 2021. <br>
                        [<a href="https://arxiv.org/abs/2103.10427">Paper</a>]
                        [<a href="https://minyoungg.github.io/overparam">Webpage</a>]
                        [<a href="https://github.com/minyoungg/overparam">GitHub</a>]
                        [<a href="https://github.com/minyoungg/overparam#3-cite">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img height="80" align="center" src="https://taesung.me/SwappingAutoencoder/index_files/church_style_swaps.gif" border="0"> &nbsp;
                        <img height="80" align="center" src="https://taesung.me/SwappingAutoencoder/index_files/tree_smaller.gif" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Swapping Autoencoder for Deep Image Manipulation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://taesung.me/">Taesung Park</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, Richard Zhang<br>
                        In NeurIPS, 2020. <br>
                        [<a href="https://arxiv.org/abs/2007.00653">Paper</a>]
                        [<a href="https://taesung.me/SwappingAutoencoder">Webpage</a>]
                        [<a href="https://github.com/taesungp/swapping-autoencoder-pytorch">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=0elW11wRNpg&feature=emb_title">Video</a>]
                        [<a href="https://taesung.me/SwappingAutoencoder/index_files/bibtex_arxiv2020.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img height="100" align="center" src="./index_files/fewshot_neurips20.jpg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Few-shot Image Generation with Elastic Weight Consolidation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://yijunmaverick.github.io/">Yijun Li</a>, Richard Zhang, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a><br>
                        In NeurIPS, 2020. <br>
                        [<a href="https://arxiv.org/abs/2012.02780">Paper</a>]
                        [<a href="https://proceedings.neurips.cc/paper/2020/file/b6d767d2f8ed5d21a44b0e5886680cb9-Supplemental.pdf">Supplemental</a>]
                        [<a href="https://yijunmaverick.github.io/publications/ewc/">Webpage</a>]
                        [<a href="./index_files/bibtex_fewshot_neurips20.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img height="100" align="center" src="index_files/eccv2020_cut.jpg" border="0"> &nbsp;
                        <!-- <img height="110" align="center" src="index_files/eccv2020_cut.jpg" border="0"> &nbsp; -->
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Contrastive Learning for Unpaired Image-to-Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://taesung.me/">Taesung Park</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, Richard Zhang, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In ECCV, 2020. <br>
                        [<a href="https://arxiv.org/abs/2007.15651">Paper</a>]
                        [<a href="http://taesung.me/ContrastiveUnpairedTranslation">Webpage</a>]
                        [<a href="https://github.com/taesungp/contrastive-unpaired-translation">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=Llg0vE_MVgk&feature=emb_title">Teaser</a>]
                        [<a href="https://www.youtube.com/watch?v=jSGOzjmN8q0">Video</a>]
                        [<a href="http://taesung.me/ContrastiveUnpairedTranslation/index_files/bibtex_eccv2020.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=left>
                        <img width="260" align="center" src="./index_files/arxiv2020_project_teaser.jpg" border="0">
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> Transforming and Projecting Images into Class-conditional Generative Networks</b><br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/minhuh/">Minyoung Huh</a>, Richard Zhang, <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="http://people.csail.mit.edu/sparis/">Sylvain Paris</a>, <a href="https://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a><br>
                        In ECCV (oral), 2020. <br>
                        <!-- [<a href="">Paper</a>] -->
                        [<a href="https://arxiv.org/abs/2005.01703">Paper</a>]
                        [<a href="https://minyoungg.github.io/pix2latent/">Webpage</a>]
                        [<a href="https://github.com/minyoungg/pix2latent">GitHub</a>]
                        [<a href="./index_files/bibtex_arxiv2020_trans_proj.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="./index_files/audio_teaser.jpg" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.cs.princeton.edu/~pmanocha/">Pranay Manocha</a>, <a href="https://www.cs.princeton.edu/~af/">Adam Finkelstein</a>, Richard Zhang, <a href="https://ccrma.stanford.edu/~njb/">Nicholas J. Bryan</a>, <a href="https://ccrma.stanford.edu/~gautham/Site/Gautham_J._Mysore.html">Gautham J. Mysore</a>, <a href="https://research.adobe.com/person/zeyu-jin/">Zeyu Jin</a><br>
                        In Interspeech, 2020. <br>
                        [<a href="https://arxiv.org/abs/2001.04460">Paper</a>]
                        [<a href="https://gfx.cs.princeton.edu/pubs/Manocha_2020_ADP/">Webpage</a>]
                        [<a href="https://github.com/pranaymanocha/PerceptualAudio">GitHub</a>]
                        [<a href="./index_files/bibtex_arxiv2020_audio.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="220" align="center" src="./index_files/cnndetect_teaser2.png" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>CNN-generated images are surprisingly easy to spot...for now</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://peterwang512.github.io">Sheng-Yu Wang</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, Richard Zhang, <a href="http://andrewowens.com">Andrew Owens</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a> <br>
                        In CVPR, 2020 (oral). <br>
                        [<a href="https://arxiv.org/abs/1912.11035">Paper</a>]
                        [<a href="https://peterwang512.github.io/CNNDetection/">Webpage</a>]
                        [<a href="https://github.com/PeterWang512/CNNDetection">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=98bCHTkp5sE">Talk</a>]
                        [<a href="https://peterwang512.github.io/CNNDetection/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="250" align="center" src="./index_files/shape_2019_teaser.jpg" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Deep Parametric Shape Predictions using Distance Fields</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/smirnov/">Dmitriy Smirnov</a>,
                        <a href="https://research.adobe.com/person/matt-fisher/">Matthew Fisher</a>,
                        <a href="https://research.adobe.com/person/vladimir-kim/">Vladimir G. Kim</a>,
                        Richard Zhang,
                        <a href="https://people.csail.mit.edu/jsolomon/">Justin Solomon</a><br>
                        In CVPR, 2020. <br>
                        [<a href="https://arxiv.org/abs/1904.08921">Paper</a>]
                        [<a href="https://people.csail.mit.edu/smirnov/deep-parametric-shapes/">Webpage</a>]
                        [<a href="https://github.com/dmsm/DeepParametricShapes">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=v_0UrjbTtHg">Video</a>]
                        [<a href="./index_files/bibtex_arxiv2019_shape.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <div style="width: 260; height: 80; margin: -10px 0px 0px 0px; overflow: hidden">
                            <img width="75" align="center" src="./index_files/morph_1028_AB.jpg_100619_AB.jpg.gif" border="0">
                            <img width="75" align="center" src="./index_files/morph_7994928.362576.jpg_7590611.3.jpg.gif" border="0">
                            <img width="100" align="center" src="./index_files/morph_3113.png_3938.png.gif" border="0">
                        </div>
                        <!-- <img width="80" align="center" src="./index_files/morph_100938_AB.jpg_27721_AB.jpg.gif" border="0"> -->
                        <!-- <img width="80" align="center" src="./index_files/morph_f58a26c915e7a1dceae7a0fa074b4a2a_1.png_7805239ad1e40e07c69d7040c52664c5_1.png.gif" border="0"> -->
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> Image Morphing with Perceptual Constraints and STN Alignment </b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.cs.tau.ac.il/~noafish/">Noa Fish</a>, Richard Zhang, Lilach Perry, <a href="https://www.cs.tau.ac.il/~dcor/index.html">Daniel Cohen-Or</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="http://www.connellybarnes.com/">Connelly Barnes</a><br>
                        In CGF, 2020. <br>
                        [<a href="https://arxiv.org/abs/2004.14071">Paper</a>]
                        [<a href="https://github.com/noafish/MorphGAN">GitHub</a>]
                        [<a href="./index_files/bibtex_arxiv2020_morph.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2019</b></span>
                </td></tr> -->
                
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="125" align="center" src="./index_files/aacnns_2019_teaser.gif" border="0">
                        <img width="125" align="center" src="./index_files/aacnns_2019_teaser2.gif" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Making Convolutional Networks Shift-Invariant Again</b> <br>
                        <span style="font-size: 10pt;">
                        Richard Zhang <br>
                        In ICML, 2019. <br>
                        <!-- [<a href="https://richzhang.github.io/antialiased-cnns/resources/camera-ready.pdf">Paper</a>] -->
                        [<a href="https://arxiv.org/abs/1904.11486">Paper</a>]
                        [<a href="https://richzhang.github.io/antialiased-cnns/">Webpage</a>]
                        [<a href="https://github.com/adobe/antialiased-cnns">GitHub</a>]
                        [<a href='https://www.youtube.com/watch?v=HjewNBZz00w&feature=youtu.be'>Talk</a>]
                        [<a href='https://www.dropbox.com/s/4mco6s76d9hi00n/antialiasing_cnns.pptx?dl=0'>Slides</a> (129mb)]
                        [<a href="https://www.dropbox.com/s/dhf2gqt14sq76q7/poster_icml.pdf?dl=0">Poster</a>]
                        [<a href="./index_files/bibtex_icml2019.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="250" align="center" src="./index_files/falteaser.png" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Detecting Photoshopped Faces by Scripting Photoshop</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://peterwang512.github.io/">Sheng-Yu Wang</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, <a href="http://andrewowens.com">Andrew Owens</a>, Richard Zhang, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a> <br>
                        In ICCV, 2019. <br>
                        [<a href="https://arxiv.org/abs/1906.05856">Paper</a>]
                        [<a href="https://peterwang512.github.io/FALdetector/">Webpage</a>]
                        [<a href="https://github.com/peterwang512/FALdetector">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=TUootD36Xm0">Video</a>]
                        [<a href="https://www.dropbox.com/s/qn4c19134zjziyi/%5BFinal%5D%20ICCV%20Poster.pdf?dl=0">Poster</a>]
                        [<a href="https://www.youtube.com/watch?v=21lj8tCSMkg">Adobe Max</a>]
                        [<a href="https://theblog.adobe.com/adobe-research-and-uc-berkeley-detecting-facial-manipulations-in-adobe-photoshop/">Adobe Blog</a>]
                        [<a href="./index_files/bibtex_fal2019.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="125" align="center" src="./index_files/isketchnfill_teaser.gif" border="0">
                        <img width="125" align="center" src="./index_files/isketchnfill_teaser2.gif" border="0">
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Interactive Sketch & Fill: Multiclass Sketch-to-Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://arnabgho.github.io/">Arnab Ghosh</a>,
                        Richard Zhang,
                        <a href="https://puneetkdokania.github.io/">Puneet Dokania</a>,
                        <a href="http://www.oliverwang.info/">Oliver Wang</a>,
                        <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
                        <a href="http://www.robots.ox.ac.uk/~phst/">Philip H.S. Torr</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>
                        In ICCV, 2019. <br>
                        [<a href="https://arxiv.org/abs/1909.11081">Paper</a>]
                        [<a href="https://arnabgho.github.io/iSketchNFill/">Webpage</a>]
                        [<a href="https://github.com/arnabgho/iSketchNFill">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=T9xtpAMUDps">Video</a>]
                        [<a href="./index_files/bibtex_iccv2019_isketchnfill.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2018</b></span>
                </td></tr> -->
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="250" align="center" src="./index_files/perceptual_teaser.jpg" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</b> <br>
                        <span style="font-size: 10pt;">
                        Richard Zhang, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a><br>
                        In CVPR, 2018. <br>
                        [<a href="http://arxiv.org/abs/1801.03924">Paper</a>]
                        [<a href="https://richzhang.github.io/PerceptualSimilarity/">Webpage</a>]
                        [<a href="https://github.com/richzhang/PerceptualSimilarity">GitHub</a>]
                        [<a href="https://richzhang.github.io/PerceptualSimilarity/index_files/poster_cvpr.pdf">Poster</a>]
                        [<a href="https://research.adobe.com/with-deep-learning-computers-see-images-more-like-humans-do/">Adobe Blog</a>]
                        [<a href="https://www.youtube.com/watch?v=DglrYx9F3UU">Two Min Papers</a>]
                        [<a href="./index_files/bibtex_cvpr2018.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="250" align="center" src="./index_files/savp3.gif" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Stochastic Adversarial Video Prediction</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.eecs.berkeley.edu/~alexlee_gk/index.html">Alex X. Lee</a>, Richard Zhang, 
                        <a href="https://febert.github.io/">Frederik Ebert</a>, <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>, <a href="https://people.eecs.berkeley.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> <br>
                        In ArXiv, 2018. <br>
                        [<a href="https://arxiv.org/abs/1804.01523">Paper</a>]
                        [<a href="https://alexlee-gk.github.io/video_prediction/">Webpage</a>]
                        [<a href="https://github.com/alexlee-gk/video_prediction">GitHub</a>]
                        [<a href="./index_files/bibtex_savp.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2017</b></span>
                </td></tr> -->
                
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="250" align="center" src="./index_files/nips2017.jpg" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Toward Multimodal Image-to-Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>, Richard Zhang, <a href="http://people.eecs.berkeley.edu/~pathak/">Deepak Pathak</a>, <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a> <br>
                        In NIPS, 2017. <br>
                        [<a href="https://arxiv.org/abs/1711.11586">Paper</a>]
                        <!-- <a href="https://papers.nips.cc/paper/6650-toward-multimodal-image-to-image-translation">Official</a>] -->
                        [<a href="https://junyanz.github.io/BicycleGAN/">Webpage</a>]
                        [<a href="https://github.com/junyanz/BicycleGAN">GitHub</a>]
                        [Video (<a href="https://www.youtube.com/watch?v=JvGysD2EFhw">YouTube</a>)(<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN/video_extended.mp4">mp4</a>)]
                        [<a href="http://junyanz.github.io/BicycleGAN/index_files/poster_nips_v3.pdf">Poster</a>]
                        [<a href="https://www.youtube.com/watch?v=XcxzKLrCpyk">Two Min Papers</a>]
                        [<a href="./index_files/bibtex_nips2017.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <a href="http://richzhang.github.io/ideepcolor"><img width="250" align="center" src="./index_files/siggraph2017_update.jpg" border="0"></a>
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Real-Time User-Guided Image Colorization with Learned Deep Priors</b> <br>
                        <span style="font-size: 10pt;">
                        Richard Zhang*, <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>*, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, <a href="http://young-geng.xyz/">Xinyang Geng</a>, <a href="http://www.cs.utexas.edu/~alin/">Angela S. Lin</a>, <a href="https://tianheyu927.github.io/">Tianhe Yu</a>, 
                        <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>        <br>
                        (*indicates equal contribution) <br>
                        In SIGGRAPH, 2017. <br>
                        [<a href="https://arxiv.org/abs/1705.02999">Paper</a>]
                        <!-- <a href="https://dl.acm.org/citation.cfm?id=3073703">Official</a>] -->
                        [<a href="https://richzhang.github.io/InteractiveColorization/">Webpage</a>]
                        [<a href="https://youtu.be/eiFzQI7LzO0?t=5690">Fastforward</a>]
                        [<a href="https://www.youtube.com/watch?v=rp5LUSbdsys">Talk</a>]
                        [Video (<a href="https://www.youtube.com/watch?v=eL5ilZgM89Q&feature=youtu.be">YouTube</a>)(<a href="https://www.dropbox.com/s/mfi66auuv7qzyx0/iColor_release.mp4?dl=0">mp4</a>)]
                        [<a href="http://video.tv.adobe.com/v/28291">PSE 2020</a>]
                        [<a href="https://github.com/junyanz/interactive-deep-colorization">GitHub</a>]
                        [<a href="https://www.dropbox.com/s/urmifx558nw0ogi/release.pptx?dl=0">Slides</a> (141mb)]
                        [<a href="./index_files/bibtex_siggraph2017.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                        <a href="http://richzhang.github.io/splitbrainauto"><img width="250" align="center" src="./index_files/cvpr2017_splitbrain.png" border="0"></a>
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</b><br>
                        <span style="font-size: 10pt;">
                        Richard Zhang, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
                        <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>        <br>
                        In CVPR, 2017.
                        <br>
                        [<a href="https://arxiv.org/abs/1611.09842">Paper</a>]
                        <!-- (<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Split-Brain_Autoencoders_Unsupervised_CVPR_2017_paper.html">official</a>)] -->
                        [<a href="http://richzhang.github.io/splitbrainauto">Webpage</a>]
                        [<a href="https://github.com/richzhang/splitbrainauto">GitHub</a>]
                        [<a href="https://richzhang.github.io/splitbrainauto/index_files/poster_cvpr.pdf">Poster</a>]
                        [<a href="https://www.youtube.com/watch?v=FTzcFsz2xqw">Seminar Talk</a>]
                        [<a href="./index_files/bibtex_cvpr2017_splitbrain.txt">Bibtex</a>]</span>
                    </td>
                </tr>


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2016</b></span>
                </td></tr> -->
                
                <tr>
                    <td width="30%" align=left>
                    <!-- height="90" -->
                        <a href="http://richzhang.github.io/colorization/"><img width="250" align="center" src="./index_files/arxiv2016_colorization.jpg" border="0"></a>
                        </td>
                        <td>
                        <span style="font-size: 12pt;"><b>Colorful Image Colorization</span></b><br>
                        <span style="font-size: 10pt;">
                        Richard Zhang, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
                        <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>        <br>
                        In ECCV, 2016 (oral).
                        <br>
                        [<a href="https://arxiv.org/abs/1603.08511">Paper</a>]
                        <!-- <a href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40">Official</a>] -->
                        [<a href="http://richzhang.github.io/colorization/">Webpage</a>]
                        [<a href="https://github.com/richzhang/colorization">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=4xoTD58Wt-0">Talk</a>]
                        [<a href="https://www.dropbox.com/s/sa8m3y1ymj0ihct/presentation_eccv_release.pptx?dl=0">Slides</a> (138mb)]
                        [<a href="http://www.eccv2016.org/files/posters/O-2B-03.pdf">Poster</a>]
                        [<a href="./index_files/bibtex_eccv2016_colorization.txt">Bibtex</a>]
                    </td>
                </tr>

<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2015</b></span>
                </td></tr> -->
                
                <tr>
					<td width="30%" align=left>
						<img width="250" align="center" src="./index_files/icra2015.png" border="0">
                    </td>
					<td>
                        <span style="font-size: 12pt;">
                        <b>Sensor Fusion for Semantic Segmentation of Urban Scenes</b><br>
                        <span style="font-size: 10pt;"> 
						Richard Zhang, <a href="https://www.linkedin.com/in/candrastefan/">Stefan Candra</a>, </a><a href="http://anp.lbl.gov/kai-vetter/">Kai Vetter</a>,
                        <a href="http://www-video.eecs.berkeley.edu/~avz/">Avideh Zakhor</a>		<br>
						In ICRA, 2015.
                        <br>
                        [Paper (<a href="./index_files/icra2015.pdf">pdf</a>)(<a href="http://ieeexplore.ieee.org/document/7139439/">official</a>)]

                        [<a href="./index_files/pres_icra2015.pdf">Slides</a>]
                        [<a href="./index_files/poster_icra2015.pdf">Poster</a>]
                        [<a href="https://www.youtube.com/watch?v=s9X63Ma3M0Y">Talk</a>]

                        [Annotations (<a href="http://www.eecs.berkeley.edu/~rich.zhang/projects/2015_ICRA_semanticsegmentation/KITTI_public.tar">tar</a>)(<a href="http://www.eecs.berkeley.edu/~rich.zhang/projects/2015_ICRA_semanticsegmentation/KITTI_public.zip">zip</a>) ]
                        [<a href="./index_files/bibtex_icra2015.txt">Bibtex</a>]
                        </span>
					</td>
				</tr>  


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2014</b></span>
                </td></tr> -->
                

                <tr>
                    <td width="30%" align="center">
                    <!-- height="100" -->
                        <img height="95" horizontal-align="center" src="./index_files/wacv2014.png" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;"><b>Automatic Identification of Window Regions on Indoor Point Clouds Using LiDAR and Cameras</b><br>
                        <span style="font-size: 10pt;">
                        Richard Zhang,
                        <a href="http://www-video.eecs.berkeley.edu/~avz/">Avideh Zakhor</a>
                        <br>
                        In WACV, 2014.
                        <br>
                        [Paper (<a href="./index_files/wacv2014.pdf">pdf</a>)(<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.649.303&rep=rep1&type=pdf">official</a>)]

                        [<a href="./index_files/bibtex_wacv2014.txt">Bibtex</a>]
                        </span>
                    </td>
                </tr>

            </tbody></table>

        <h2>Thesis </h2>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
                <tbody>
                <tr>
                    <td width="30%" align=left>
                        <img width="250" align="center" src="./index_files/berkeley_logo.png" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Image Synthesis for Self-Supervised Visual Representation Learning</b> <br>
                        <span style="font-size: 10pt;">
                        Richard Zhang<br>
                        <!-- Commitee: Alexei A. Efros, Trevor Darrell, Michael DeWeese.<br> -->
                        Spring 2018.<br>
                        [<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-36.html">Thesis</a>]
                        [<a href="https://www.youtube.com/watch?v=aGhYitrOJRc">Dissertation Talk</a>]
                        [<a href="https://youtu.be/IXG-uWFAkmM">Fast Forward</a>]
                        [<a href="https://www.dropbox.com/s/96f0xhfwvjnbf52/presentation_dissertation.pptx?dl=0">Slides</a> (396 MB)]
                        [<a href="./index_files/bibtex_thesis.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
            </tbody></table>

        <h2>Awards </h2>
        <span style="font-size: 10pt;">
        Reviewer recognitions, CVPR 2019, NeurIPS 2019, ECCV 2020, NeurIPS 2020<br>
        Thesis Fast Forward, Best Presentation, SIGGRAPH 2018<br>
        <a href="https://research.adobe.com/fellowship/">Adobe Research Fellowship</a> 2017
        </span>
<!--         <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="25">
                <tbody>
                <tr>
                    <td width="30%" align="center">
                        <img height="75" horizontal-align="center" src="./index_files/Adobe-logo.png" border="0">
                            </td>
                    <td>
                        <a href="https://research.adobe.com/fellowship/">Adobe Research Fellowship</a> 2017
                    </td>
                </tr>
            </tbody></table>
        </font> -->
        <br>

        <h2>Student collaborators/interns</h2>
        I have gotten to work with some wonderful collaborators.<br>

        <!-- <h3>Internship </h3> -->
<!--         <br>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <span style="font-size: 10pt;">
            If you have similar interests and are interested in collaborating during a summer 2021 internship, please feel free to contact me! Tell me about your past research experience and what you would potentially like to do. The goal of an internship is a publication, usually CVPR or SIGGRAPH. For example, see my NIPS 2017 and CVPR 2018 papers, which were from my summer 2017 internship. Interns are almost all PhD students; the number of slots is limited, so we unfortunately cannot accept everyone.<br>
            </span>
        </font> -->

        <!-- <h3>@Adobe</h3> -->
        <br>
         <span style="font-size: 14pt;">
            <b>@Adobe</b>
        </span>

        <span style="font-size: 10pt;">
        <!-- <p> -->
        <dl class="dl-horizontal">
            <dt><b>PhD/MS [interns]</b></dt>
            <a href="http://peterwang512.github.io/">Sheng-Yu Wang</a>, CMU <br>
            Xiaojuan Wang, UW <br>
            <a href="https://yossigandelsman.github.io/">Yossi Gandelsman</a>, UC Berkeley <br>
            <a href="https://nupurkmr9.github.io//">Nupur Kumari</a>, CMU<br>
            Minguk Kang, POSTTECH<br>
            <a href="https://dave.ml/">Dave Epstein</a>, UC Berkeley<br>
            <a href="https://gauravparmar.com/">Gaurav Parmar</a>, CMU<br>
            <a href="https://yotamnitzan.github.io/">Yotam Nitzan</a>, UW<br>
            <a href="https://homes.cs.washington.edu/~royorel/">Roy Or-El</a>, UW<br>
            <a href="http://people.csail.mit.edu/lrchai/">Lucy Chai</a>, MIT (Fellowship winner, 2021)<br>
            <a href="https://lychenyoko.github.io/">Yuchen Liu</a>, Princeton<br>
            <a href="https://people.cs.umass.edu/~dliu/">Difan Liu</a>, UMass Amherst<br>
            <a href="https://taesung.me/">Taesung Park</a>, UC Berkeley (Fellowship winner, 2020)<br>
            <a href="http://linji.me/">Ji Lin</a>, MIT<br>
            <a href="https://www.wpeebles.com/">William (Bill) Peebles</a>, UC Berkeley<br>
            <a href="https://www.alexandonian.com/">Alex Andonian</a>, MIT<br>
            <a href="https://utkarshojha.github.io/">Utkarsh Ojha</a>, UC Davis<br>
            <a href="http://arnabgho.github.io/">Arnab Ghosh</a>, Oxford<br>
            <a href="http://minyounghuh.com/">Minyoung (Jacob) Huh</a>, MIT<br>
            <a href="https://stamarot.webgr.technion.ac.il/">Tamar Rott Shaham</a>, Technion (Fellowship winner, 2020)<br>
            <a href="https://payeah.net">Peiye Zhuang</a>, UIUC<br>
            <a href="http://people.csail.mit.edu/smirnov/">Dima Smirnov</a>, MIT<br>
            <a href="https://www.cs.tau.ac.il/~noafish/">Noa Fish</a>, Tel Aviv<br>
            <br>

            <dt><b>Masters/Undergrad [interns]</b></dt>
            <a href="http://people.csail.mit.edu/stevenliu/">Steven Liu</a>, MIT<br>
            <a href="https://sjooyoo.github.io/">Seungjoo Yoo</a>, Korea Univ (WIT scholarship winner, 2019)<br>
            <br>

            <dt><b>PhD/MS [university collaborators]</b></dt>
            <a href="https://www.cs.princeton.edu/~pmanocha/">Pranay Manocha</a>, Princeton<br>
            <a href="https://rawanmg.github.io/">Rawan Alghofaili</a>, George Mason<br>
            <a href="http://alvinwan.com/">Alvin Wan</a>, UC Berkeley<br>
            <br>

            <!-- <dt><b>Undergrad [university collaborators]</b></dt> -->
            <!-- <a href="http://peterwang512.github.io/">Sheng-Yu Wang</a>, UC Berkeley <br> -->
        </dl>

        </span>

        <!-- <h3>@Berkeley</h3> -->
        <!-- <br> -->
         <span style="font-size: 14pt;">
            <b>@Berkeley</b>
        </span>

        <span style="font-size: 10pt;">
        <dl class="dl-horizontal">
          <dt><b>Undergraduates</b></dt>
            <a href="https://www.linkedin.com/in/xin-qin-4a83b9158/">Xin Qin</a>, now @ USC <br>
            <a href="https://www.linkedin.com/in/hemangjangle/">Hemang Jangle</a> <br>
            <a href="http://www.cs.utexas.edu/~alin/">Angela S. Lin</a>, now @ UT Austin <br>
            <a href="http://young-geng.xyz/">Xinyang Geng</a>, now @ UC Berkeley<br>
            <a href="https://tianheyu927.github.io/">Tianhe Yu</a>, now @ Stanford<br>
            <a href="https://www.linkedin.com/in/candrastefan/">Stefan A. Candra</a>
        </dl>

        <!-- </p> -->
        </span>
        <!-- </p><hr size="2" align="left" noshade=""> -->


        <h2>Teaching </h2>
        <!-- <a href="https://research.adobe.com/fellowship/">Adobe Research Fellowship</a> 2017 -->
        <span style="font-size: 12pt;">
        <b>Introduction to Artificial Intelligence (CS 188)</b>, UC Berkeley <br>
        <span style="font-size: 10pt;">
        Graduate Student Instructor (GSI) with Prof. <a href="http://people.eecs.berkeley.edu/~anca/">Anca Dragan</a> <br>
        Spring 2017<br>
        <br>
        <span style="font-size: 12pt;">
        <b>Computer Vision (CS 280)</b>, UC Berkeley <br>
        <span style="font-size: 10pt;">
        Graduate Student Instructor (GSI) with Prof. <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, Prof. <a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> <br>
        Spring 2016<br>
        <br>
        <span style="font-size: 12pt;">
        <b>Introduction to Circuits (ECE 2100)</b>, Cornell University <br>
        <span style="font-size: 10pt;">
        Teaching Assistant (TA) with Prof. <a href="https://molnargroup.ece.cornell.edu/">Alyosha Molnar</a> <br>
        Spring 2010<br>
        <br>

        <h2>My Name</h2>
        Confused by the contents of this page? Well, you may have been looking for Professor <a href="https://www.cs.sfu.ca/~haoz/" border="0">Richard Zhang</a> or Professor <a href="https://ryz.ece.illinois.edu/" border="0">Richard Zhang</a>. My Chinese name is 章睿嘉.

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75897335-1', 'auto');
  ga('send', 'pageview');
</script>
            
</font></td></tr></tbody></table><iframe frameborder="0" scrolling="no" style="border: 0px; display: none; background-color: transparent;"></iframe><div id="GOOGLE_INPUT_CHEXT_FLAG" input="null" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}" style="display: none;"></div></body></html>

