
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252" charset="utf-8">
        <title>Richard Zhang - Research Scientist, Adobe Research </title>
<style type="text/css"></style></head>
    <body><table border="0" width="980px" align="center"><tbody><tr><td>
    
        </td><td valign="top">
<!--            <img src="images/cmuscslogo.gif">
                <img src="images/rilogo.png"> -->
        <br>
        <table style="font-size: 11pt;" border="0" width="100%">
            <tbody><tr>
                <td width="50%">
                    <!-- <img width="300" src="./index_files/mypic.jpeg" border="0"> -->
                    <!-- <img width="300" src="./index_files/mypic2.jpg" border="0"> -->
                    <!-- <img width="250" src="./index_files/mypic3.jpg" border="0"> -->
                    <img width="250" src="./index_files/mypic4.jpg" border="0">
                </td>
                <td>
                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="6"> 
                        <b>Richard Zhang</b><br><br>
                    </font>
                    <font face="helvetica , ariel, &#39;sans serif&#39;" size="4"> 
                        Senior Research Scientist<br>
                        Adobe Research<br>
                        San Francisco, CA<br><br>
                        rizhang at adobe.com<br>
                        [<a href="https://github.com/richzhang" border="0">GitHub</a>]
                        [<a href="https://scholar.google.com/citations?user=LW8ze_UAAAAJ&hl=en" border="0">Google Scholar</a>]<br>
                        [<a href="index_files/CV.pdf" border="0">Resume/CV</a>]
                        [<a href="https://twitter.com/rzhang88" border="0">Twitter</a>]
                        [<a href="bio.txt" border="0">Bio</a>]<br>
                    </font>
                </td>
            </tr>
        </tbody></table> 
        <p>
        </p><hr size="2" align="left" noshade="">
        <p>
        
        <font face="helvetica, ariel, &#39;sans serif&#39;">
        <!--</font></p><h2><font face="helvetica, ariel, &#39;sans serif&#39;">About Me</font></h2><font face="helvetica, ariel, &#39;sans serif&#39;">-->
        My research interests are in computer vision, machine learning, deep learning, graphics, and image processing. I obtained a PhD at UC Berkeley, advised by Prof. <a href="http://www.eecs.berkeley.edu/~efros/">Alexei (Alyosha) Efros</a>. I obtained BS and MEng degrees from Cornell University in ECE. I often collaborate with academic researchers, either through internships or university collaboration.<br><br>

        <b>Recent media.</b>
        I was included on MIT Technology Review's list of <a href="https://www.technologyreview.com/innovator/richard-zhang/">Innovators Under 35</a>. Please see this Adobe <a href="https://blog.adobe.com/en/publish/2023/09/12/adobe-research-scientist-named-top-innovator-under-35-mit-technology-review">blog post</a>, <a href="https://www.youtube.com/watch?v=YQW32sf9noE">overview video</a> (5 min), or <a href="https://twimlai.com/podcast/twimlai/visual-generative-ai-ecosystem-challenges/">TWiML podcast</a> below (40 min) for more on our work on perception, generation, and forensics for "GenAI".<br>

        <br>
        <iframe allow="autoplay *; encrypted-media *; fullscreen *; clipboard-write" frameborder="0" height="175" style="width:100%;max-width:660px;overflow:hidden;border-radius:10px;" sandbox="allow-forms allow-popups allow-same-origin allow-scripts allow-storage-access-by-user-activation allow-top-navigation-by-user-activation" src="https://embed.podcasts.apple.com/us/podcast/visual-generative-ai-ecosystem-challenges-with-richard/id1116303051?i=1000635452895"></iframe>

        <!-- <br><br> -->
        </p><hr size="2" align="left" noshade="">

        <h3>News </h3>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <span style="font-size: 10pt;">
<!--             <b>[Sept 2023]</b> I was included on a list of <a href="https://www.technologyreview.com/innovator/richard-zhang/">35 Innovators Under 35</a> by MIT Technology Review! Please see <a href="https://blog.adobe.com/en/publish/2023/09/12/adobe-research-scientist-named-top-innovator-under-35-mit-technology-review">this article</a> by Adobe and <a href="https://www.youtube.com/watch?v=YQW32sf9noE">this 5 min overview video</a> for more information.<br> -->
            <!-- <b>[Nov 2023]</b> I appeared on the <a href="https://twimlai.com/podcast/twimlai/visual-generative-ai-ecosystem-challenges/">TWiML podcast</a>, discussing building a healthy GenAI ecosystem for creators, consumers, and contributors. Links: <a href="https://open.spotify.com/episode/6V8zgyf9f6Q6p7rdRhrbpU?si=6670478bdffd46b5">Spotify</a>, <a href="https://podcasts.apple.com/us/podcast/visual-generative-ai-ecosystem-challenges-with-richard/id1116303051?i=1000635452895">Apple</a>, <a href="https://dcs.megaphone.fm/MLN6292733087.mp3">File</a> (40 min); <a href="https://www.youtube.com/watch?v=PdHmoqS70r0">YouTube</a> (51 min).<br> -->
            <b>[Sept 2024]</b> Attribution by Unlearning and DMDv2 were accepted to NeurIPS 2024.<br>
            <b>[Aug 2024]</b> Check out TurboEdit, accepted to ECCV 2024.<br>
            <b>[Jul 2024]</b> Diffusion2GAN, Lazy Diffusion, and Editable Image Elements were accepted to ECCV 2024.<br>
            <b>[Apr 2024]</b> I'll be speaking at <a href="https://gamgc.github.io/">GenAI Media Challenge</a> and <a href="https://fadetrcv.github.io/2024/">Fair, Data-Efficient, and Trusted Computer Vision</a> workshops.<br>
            <b>[Feb 2024]</b> DMD was accepted to CVPR 2024.<br>
            <b>[Sept 2023]</b> DreamSim was accepted to NeurIPS 2023 as a spotlight.<br>
            <!-- <b>[Sept 2023]</b> I was included on MIT Technology Review's list of <a href="https://www.technologyreview.com/innovator/richard-zhang/">35 Innovators Under 35</a>.<br> -->
            <!-- <b>[Jul 2023]</b> Data Attribution and Concept Ablation were accepted to ICCV 2023.<br> -->
            <!-- <b>[Jun 2023]</b> Pix2Pix-0 was accepted to SIGGRAPH 2023.<br> -->
            <!-- <b>[Mar 2023]</b> GigaGAN was selected as a highlight for CVPR 2023.<br> -->
            <!-- <b>[Feb 2023]</b> GigaGAN, Custom Diffusion, and Domain Expansion were accepted to CVPR 2023.<br> -->
            <!-- <b>[Oct 2022]</b> I am presenting some posters at ECCV. BlobGAN on Tue, Oct 25, 1100-1330, 3D-FM GAN at 1530-1730, and Anyres-GAN on Wed, Oct 26, 1100-1330. See you there!<br> -->
            <!-- <b>[Oct 2022]</b> I am co-organizing the <a href="https://she-workshop.github.io/">Sketching for Human Expressivity Workshop</a> at ECCV on Sun, Oct 23rd. See you there!<br> -->
            <!-- <b>[Sept 2022]</b> çˆ·çˆ·å¥½ðŸ‘‹<br> -->
            <!-- <b>[Jul 2022]</b> Any-Resolution GANs, BlobGAN, and 3D-FM GAN were accepted to ECCV 2022.<br> -->
            <!-- <b>[Jun 2022]</b> Our GANgealing work was one of the 33 best paper finalists at CVPR 2022.<br> -->
            <!-- <b>[Jun 2022]</b> I spoke about "Anycost and Anyres GANs for Image Synthesis" at the <a href="https://data.vision.ee.ethz.ch/cvl/ntire22/">NTIRE</a> and <a  href="https://ai4cc.net/">AICC</a> CVPR 2022 workshops.<br> -->
            <!-- <b>[May 2022]</b> Please see our SIGGRAPH 2022 work, ASSET, which enables high-resolution semantic editing with transformers.<br> -->
            <!-- <b>[Apr 2022]</b> Our work on GANgealing was covered by <a href="https://www.youtube.com/watch?v=qtOkktTNs-k">2-minute papers</a>.<br> -->
            <!-- <b>[Mar 2022]</b> Our works on GANgealing and Vision-Aided GANs were accepted to CVPR as oral presentations.<br> -->
            <!-- <b>[Mar 2022]</b> Our clean-fid work was accepted to CVPR. <mark><font face="Courier">pip install clean-fid</font></mark> to try it out!<br> -->
            <!-- <b>[Oct 2021]</b> See Landscape mixer in Photoshop Neural Filters, based on our Swapping Autoencoder work.<br> -->
            <!-- <b>[July 2021]</b> Our work on editing NeRFs was accepted to ICCV.<br> -->
            <!-- <b>[Apr 2021]</b> Our work on using generative models to improve discriminative models was accepted to CVPR.<br> -->
            <!-- <b>[Apr 2021]</b> Our work on few-shot GAN training was accepted to CVPR.<br> -->
            <!-- <b>[Mar 2021]</b> Our works on speeding up unconditional GANs for image editing and projection (AnyCost GANs) and conditional GANs with implicit functions (ASAPnet) were accepted to CVPR.<br> -->
            <!-- <b>[Mar 2021]</b> Our work on audio perceptual metrics was accepted to ICASSP.<br> -->
            <!-- <b>[Sept 2020]</b> Few updates regarding <a href="https://richzhang.github.io/antialiased-cnns/">Antialiasing CNNs</a> [ICML 2019], which can <b>stabilize and improve the backbone for your application</b>:<br>
            - Easy installation: <mark><font face="Courier">pip install antialiased-cnns</font></mark> and 
            <mark><font face="Courier" color="red">import</font>
            <font face="Courier">antialiased_cnns; model</font>
            <font face="Courier" color="blue"> = </font>
            <font face="Courier">antialiased_cnns.</font><font face="Courier" color="#6c53b5">resnet50</font><font face="Courier">(pretrained=</font>
            <font face="Courier" color="blue">True)</font></mark><br>
            - For more information, including "What is Aliasing?", see my <a href="https://youtu.be/8CXrplBG-SE?t=1049">guest lecture</a> [15 min] in SFU CMPT 361, Intro to Vision, Sampling and Aliasing lecture.<br>
            - A nice followup work,
            <a href="https://maureenzou.github.io/ddac/">Delving Deeper into Antialiasing in Convnets</a> by Zou, Xiao, Yu, & Lee, won best paper at BMVC 2020. Check it out!<br> -->
            <!-- <b>[Aug 2020]</b> I gave a talk on <a href="https://www.youtube.com/watch?v=CYdYWeTE-CI">Detecting Generated Imagery, Deep and Shallow</a> (35 min) at the <a href="https://sense-human.github.io/">Sensing Humans</a> workshop at ECCV.<br> -->
            <!-- <b>[Aug 2020]</b> I gave a talk on <a href="https://www.youtube.com/watch?v=aM86tOniH90">Style and Structure Disentanglement for Image Manipulation</a> (30 min) at the <a href="https://data.vision.ee.ethz.ch/cvl/aim20/">Advances in Image Manipulation</a> workshop at ECCV.<br> -->
            <!-- <b>[Aug 2020]</b> I gave a talk on <a href="https://www.bilibili.com/video/BV1e7411c7kR?p=46">Analyzing Artifacts in Discriminative and Generative Models</a> (40 min) at the GAMES webinar.<br> -->
            <!-- <b>[July 2020]</b> Our work on using contrastive learning for unpaired translation was accepted to ECCV.<br> -->
            <!-- <b>[July 2020]</b> Our work on inverting GANs was accepted to ECCV as an oral.<br> -->
            <!-- <b>[July 2020]</b> See our new work on Swapping Autoencoders below.<br> -->
            <!-- <b>[July 2020]</b> Our work on audio perceptual metrics was accepted to Intespeech.<br> -->
            <!-- <b>[Feb 2020]</b> I served as an Area Chair for CVPR 2020 and spoke on <a href="https://www.youtube.com/watch?v=aNDwHRxWTa0">Analyzing CNN Artifacts in Discriminative and Generative Models</a> (11 min). The second half includes our "Detecting CNN-generated images" work, just accepted to CVPR.<br> -->
            <!-- <b>[Dec 2019]</b> See our new work on detecting CNN-generated images below.<br> -->
            <!-- <b>[Nov 2019]</b> I presented our "Detecting Photoshop" ICCV19 work at <a href="https://www.youtube.com/watch?v=21lj8tCSMkg">Adobe MAX</a> (5 min), on stage with John Mulaney (aka Peter Porker/Spider-Ham)!<br> -->
            <!-- <b>[Oct 2019]</b> Thank you <a href="https://twitter.com/Oxford_VGG/status/1184087868857290752">Oxford</a> and UCL for hosting me.<br> -->
            <!-- <b>[Oct 2019]</b> This <a href="http://video.tv.adobe.com/v/28291">video</a> shows interactive colorization in Photoshop Elements 2020, based on our SIGGRAPH 2017 work.<br> -->
            <!-- <b>[Sept 2019]</b> See our new work on interactive sketch to image synthesis below.<br> -->
            <!-- <b>[Jun 2019]</b> See our new work on detecting Photoshopped images below.<br> -->
            <!-- <b>[May 2019]</b> Our work on anti-aliasing convolutional networks has been accepted to ICML 2019. Try anti-aliasing your convnet <a href="https://github.com/adobe/antialiased-cnns">here</a>!<br> -->
            <!-- <b>[Aug 2018]</b> I will be presenting at the Thesis Fast Forward session at SIGGRAPH on Tuesday 8/14, 2:00pm.<br> -->
            <!-- <b>[Jun 2018]</b> We will be presenting our <a href="https://richzhang.github.io/PerceptualSimilarity/">project</a> on perceptual metrics at CVPR, Tuesday 6/19, 10:10am. Try our metric <a href="https://github.com/richzhang/PerceptualSimilarity">here</a>!<br> -->
            <!-- <b>[May 2018]</b> I have <a href="./index_files/graduation.jpg">graduated</a> from UC Berkeley and have joined Adobe Research as a Research Scientist in San Francisco! -->
            </span>

<!--         </p><hr size="2" align="left" noshade="">

        <h3>Internship </h3>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <span style="font-size: 10pt;">
            If you have similar interests and are interested in collaborating during a Summer 2023 internship, I'd be happy to hear from you! <b>Please apply <a href="https://research.adobe.com/careers/internships/">here</a> first</b>. Tell me about your past research experience and what you would potentially like to do. The goal of an internship is a publication, usually CVPR or SIGGRAPH. Interns are typically PhD students; the number of slots is limited, so we unfortunately cannot accept everyone.
            </span>
        </font>
 -->
        </p><hr size="2" align="left" noshade="">

        <h2>Publications </h2>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
				<tbody>
                <tr>
                    <td width="30%" align=center>
                        <video width="120" class="video lazy" autoplay loop playsinline controls muted>
                            <source src="https://huggingface.co/datasets/tianweiy/causvid_website/resolve/main/videos/long_videos/60s/0024.mp4" type="video/mp4"></source></video>
                        <video width="120" class="video lazy" autoplay loop playsinline controls muted>
                            <source src="https://huggingface.co/datasets/tianweiy/causvid_website/resolve/main/videos/i2v/0007.mp4" type="video/mp4"></source></video>
                    </td>                    
                    <td>
                        <span style="font-size: 12pt;">
                        <b>From Slow Bidirectional to Fast Causal Video Generators</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://tianweiy.github.io/">Tianwei Yin</a>, 
                        <a href="https://www.linkedin.com/in/qiang-zhang-6b48791a7/">Qiang Zhang</a>, 
                        Richard Zhang,
                        <a href="https://billf.mit.edu/">William T. Freeman</a>, 
                        <a href="https://people.csail.mit.edu/fredo/">Fredo Durand</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://www.xunhuang.me/">Xun Huang</a>
                        <br>
                        In ArXiv, 2024. <br>
                        [<a href="https://arxiv.org/abs/2412.07772">Paper</a>]
                        [<a href="https://causvid.github.io/">Webpage</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="200" align="center" src="https://peterwang512.github.io/AttributeByUnlearning/files/teaser.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Data Attribution for Text-to-Image Models by Unlearning Synthesized Images</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://peterwang512.github.io">Sheng-Yu Wang</a>,
                        <a href="https://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a>,
                        <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                        Richard Zhang
                        <br>
                        In NeurIPS, 2024. <br>
                        [<a href="https://arxiv.org/abs/2406.09408">Paper</a>]
                        [<a href="https://peterwang512.github.io/AttributeByUnlearning/">Webpage</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="250" align="center" src="https://tianweiy.github.io/dmd2/static/images/pipeline.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Improved Distribution Matching Distillation for Fast Image Synthesis</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://tianweiy.github.io/">Tianwei Yin</a>, 
                        <a href="http://mgharbi.com/">Michael Gharbi</a>, 
                        <a href="https://taesung.me/">Taesung Park</a>, 
                        Richard Zhang,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://people.csail.mit.edu/fredo/">Fredo Durand</a>,
                        <a href="https://billf.mit.edu/">William T. Freeman</a>
                        <br>
                        In NeurIPS (oral), 2024. <br>
                        [<a href="https://arxiv.org/abs/2405.14867">Paper</a>]
                        [<a href="https://tianweiy.github.io/dmd2/">Webpage</a>]
                        [<a href="https://tianweiy.github.io/dmd2/#bibtex">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="240" align="center" src="index_files/teaser_customdiffusion360.jpeg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Customizing Text-to-Image Diffusion with Camera Viewpoint Control</b><br>
                        <span style="font-size: 10pt;">
                        <a href="https://nupurkmr9.github.io/">Nupur Kumari</a>,
                        <a href="https://graceduansu.github.io/">Grace Su</a>,
                        Richard Zhang,
                        <a href="https://taesung.me/">Taesung Park</a>, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>
                        <br>
                        In SIGGRAPH Asia, 2024. <br>
                        [<a href="https://arxiv.org/abs/2404.12333">Paper</a>]
                        [<a href="https://customdiffusion360.github.io/">Webpage</a>]
                        [<a href="https://github.com/customdiffusion360/custom-diffusion360">GitHub</a>]
                        [<a href="https://huggingface.co/spaces/customdiffusion360/customdiffusion360">Demo</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <video width="70" class="video lazy" autoplay loop playsinline controls muted>
                            <source src="https://joaanna.github.io/customizing_motion/static/videos/many/carlton/An_older_lady_doing_the_pnn_dance_while_jumping_up_and_down_._seed49954.mp4" type="video/mp4"></source></video>
                        <video width="70" class="video lazy" autoplay loop playsinline controls muted>
                            <source src="https://joaanna.github.io/customizing_motion/static/videos/many/carlton/nurses_dancing_the_sks_dance_in_a_hospital_seed0.mp4" type="video/mp4"></source></video>
                        <video width="70" class="video lazy" autoplay loop playsinline controls muted>
                            <source src="https://joaanna.github.io/customizing_motion/static/videos/many/carlton/A_toddler_giggling_while_attempting_the_sks_dance_in_the_living_room._seed0.mp4" type="video/mp4"></source>
                        </video>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>NewMove: Customizing text-to-video models with novel motions</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://joaanna.github.io/">Joanna Materzynska</a>, 
                        <a href="http://people.ciirc.cvut.cz/~sivic/">Josef Sivic</a>, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
                        Richard Zhang,
                        <a href="https://bryanrussell.org/">Bryan Russell</a>
                        <br>
                        In ACCV, 2024. <br>
                        [<a href="https://arxiv.org/abs/2312.04966">Paper</a>]
                        [<a href="https://joaanna.github.io/customizing_motion/">Webpage</a>]
                        [<a href="index_files/bibtex_arxiv23_motioncust.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="75" align="center" src="https://betterze.github.io/TurboEdit/img/black2white.gif" border="0"> 
                        <img width="75" align="center" src="https://betterze.github.io/TurboEdit/img/short.gif" border="0"> 
                        <img width="75" align="center" src="https://betterze.github.io/TurboEdit/img/pharaoh2fox.gif" border="0"> 
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>TurboEdit: Instant text-based image editing</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://scholar.google.com/citations?user=V8FwQGkAAAAJ&hl=en">Zongze Wu</a>, 
                        <a href="https://home.ttic.edu/~nickkolkin/home.html">Nicholas Kolkin</a>, 
                        <a href="https://www.linkedin.com/in/jonathan-brandt-23b334/">Jonathan Brandt</a>, 
                        Richard Zhang, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>
                        <br>
                        In ECCV, 2024. <br>
                        [<a href="https://arxiv.org/abs/2408.08332">Paper</a>]
                        [<a href="https://betterze.github.io/TurboEdit/">Webpage</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="250" align="center" src="https://mingukkang.github.io/Diffusion2GAN/static/images/G_architecture.png" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Diffusion2GAN: Distilling Diffusion Models into Conditional GANs</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://mingukkang.github.io/">Minguk Kang</a>, 
                        Richard Zhang, 
                        <a href="https://www.connellybarnes.com/work/">Connelly Barnes</a>, 
                        <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a>, 
                        <a href="https://suhakwak.github.io/">Suha Kwak</a>, 
                        <a href="https://jaesik.info/">Jaesik Park</a>, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, 
                        <a href="https://taesung.me/">Taesung Park</a>
                        <br>
                        In ECCV, 2024. <br>
                        [<a href="https://arxiv.org/abs/2405.05967">Paper</a>]
                        [<a href="https://mingukkang.github.io/Diffusion2GAN/">Webpage</a>]
                        [<a href="https://mingukkang.github.io/Diffusion2GAN#bibtex">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <video width="150" class="video lazy" autoplay loop playsinline controls muted>
                            <source src="https://jitengmu.github.io/Editable_Image_Elements/static/images/teaser_combine.mp4" type="video/mp4"></source></video>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Editable Image Elements for Controllable Synthesis</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://jitengmu.github.io/">Jiteng Mu</a>, 
                        <a href="http://mgharbi.com/">Michael Gharbi</a>, 
                        Richard Zhang, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="http://www.svcl.ucsd.edu/~nuno/">Nuno Vasconcelos</a>, 
                        <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>, 
                        <a href="https://taesung.me/">Taesung Park</a>, 
                        <br>
                        In ECCV, 2024. <br>
                        [<a href="https://arxiv.org/abs/2404.16029">Paper</a>]
                        [<a href="https://jitengmu.github.io/Editable_Image_Elements/">Webpage</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="240" align="center" src="https://lazydiffusion.github.io/static/images/teaser.png" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Lazy Diffusion Transformer for Interactive Image Editing</b><br>
                        <span style="font-size: 10pt;">
                        <a href="https://yotamnitzan.github.io/">Yotam Nitzan</a>, 
                        <a href="https://www.cs.huji.ac.il/w~wuzongze/">Zongze Wu</a>, 
                        Richard Zhang,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>, 
                        <a href="https://taesung.me/">Taesung Park</a>, 
                        <a href="http://mgharbi.com/">Michael Gharbi</a>
                        <br>
                        In ECCV, 2024. <br>
                        [<a href="https://arxiv.org/abs/2404.12382">Paper</a>]
                        [<a href="https://lazydiffusion.github.io/">Webpage</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="160" align="center" src="index_files/moe_eccv24_teaser.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Mixture of Efficient Diffusion Experts Through Automatic Interval and Sub-Network Selection</b><br>
                        <span style="font-size: 10pt;">
                        <a href="https://alii-ganjj.github.io/">Alireza Ganjdanesh</a>, 
                        <a href="https://research.adobe.com/person/yan-kang/">Yan Kang</a>, 
                        <a href="https://lychenyoko.github.io/">Yuchen Liu</a>, 
                        Richard Zhang, 
                        <a href="https://research.adobe.com/person/zhe-lin/">Zhe Lin</a>, 
                        <a href="https://www.cs.umd.edu/people/heng">Heng Huang</a>
                        <br>
                        In ECCV, 2024. <br>
                        [<a href="https://arxiv.org/abs/2409.15557">Paper</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="225" align="center" src="index_files/stereo_teaser.jpeg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>What Makes for a Good Stereoscopic Image?</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://netanel-tamir.github.io/">Netanel Y. Tamir</a>, 
                        <a href="https://scholar.google.com/citations?user=zVzuR4gAAAAJ&hl=en">Shir Amir</a>, 
                        <a href="https://www.linkedin.com/in/ranel-itzhaky-847070254/?originalSubdomain=il">Ranel Itzhaky</a>, 
                        <a href="https://www.linkedin.com/in/noam-atia/?originalSubdomain=il">Noam Atia</a>, 
                        <a href="https://ssundaram21.github.io/">Shobhita Sundaram</a>, 
                        <a href="https://stephanie-fu.github.io/">Stephanie Fu</a>, 
                        <a href="https://www.linkedin.com/in/ronus/?originalSubdomain=il">Ron Sokolovsky</a>, 
                        <a href="https://web.mit.edu/phillipi/">Phillip Isola</a>, 
                        <a href="https://www.weizmann.ac.il/math/dekel/home">Tali Dekel</a>, 
                        Richard Zhang, 
                        <a href="https://www.linkedin.com/in/miriam-farber-1a07bb145/?originalSubdomain=il">Miriam Farber</a>
                        <br>
                        In Arxiv, 2024. <br>
                        [<a href="https://arxiv.org/abs/2412.21127">Paper</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="150" align="center" src="index_files/teaser_dmd.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>One-step Diffusion with Distribution Matching Distillation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://tianweiy.github.io/">Tianwei Yin</a>, 
                        <a href="http://mgharbi.com/">Michael Gharbi</a>, 
                        Richard Zhang,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://people.csail.mit.edu/fredo/">Fredo Durand</a>,
                        <a href="https://billf.mit.edu/">William T. Freeman</a>,
                        <a href="https://taesung.me/">Taesung Park</a>
                        <br>
                        In CVPR, 2024. <br>
                        [<a href="https://arxiv.org/abs/2311.18828">Paper</a>]
                        [<a href="https://tianweiy.github.io/dmd/">Webpage</a>]
                        [<a href="https://www.youtube.com/watch?v=3vo6mzk9K4s&list=TLGGlJBiJ80Rt-wwMTEyMjAyMw">Teaser</a>]
                        [<a href="index_files/bibtex_dmd2023.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="250" align="center" src="index_files/personalized_res.jpeg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Personalized Residuals for Concept-Driven Text-to-Image Generation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://cusuh.github.io/">Cusuh Ham</a>, 
                        <a href="https://techmatt.github.io/">Matthew Fisher</a>, 
                        <a href="https://faculty.cc.gatech.edu/~hays/">James Hays</a>, 
                        <a href="https://home.ttic.edu/~nickkolkin/home.html">Nick Kolkin</a>, 
                        <a href="https://lychenyoko.github.io/">Yuchen Liu</a>, 
                        Richard Zhang,
                        <a href="https://www.tobiashinz.com/">Tobias Hinz</a>
                        <br>
                        In CVPR, 2024. <br>
                        [<a href="https://arxiv.org/abs/2405.12978">Paper</a>]
                        [<a href="https://cusuh.github.io/personalized-residuals/">Webpage</a>]
                        [<a href="https://cusuh.github.io/personalized-residuals/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="250" align="center" src="https://yinboc.github.io/infd/assets/method.png" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Image Neural Field Diffusion Models</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://yinboc.github.io/">Yinbo Chen</a>, 
                        <a href="https://oliverwang.nfshost.com/">Oliver Wang</a>, 
                        Richard Zhang, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="https://xiaolonw.github.io/">Xiaolong Wang</a>, 
                        <a href="http://www.mgharbi.com/">Michael Gharbi</a>
                        <br>
                        In CVPR, 2024 (highlight). <br>
                        [<a href="https://arxiv.org/abs/2406.07480">Paper</a>]
                        [<a href="https://yinboc.github.io/infd/">Webpage</a>]
                        <!-- [<a href="https://cusuh.github.io/personalized-residuals/resources/bibtex.txt">Bibtex</a>] -->
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="200" align="center" src="https://twizwei.github.io/assets/images/videogigagan_teaser.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>VideoGigaGAN: Towards Detail-rich Video Super-Resolution</b><br>
                        <span style="font-size: 10pt;">
                        <a href="https://twizwei.github.io/">Yiran Xu</a>,
                        <a href="https://taesung.me/">Taesung Park</a>, 
                        Richard Zhang,
                        <a href="https://research.adobe.com/person/yang-zhou/">Yang Zhou</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="https://pages.cs.wisc.edu/~fliu/">Feng Liu</a>, 
                        <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a>, 
                        <a href="https://difanliu.github.io/">Difan Liu</a>
                        <br>
                        In ArXiv, 2024. <br>
                        [<a href="https://arxiv.org/abs/2404.12388">Paper</a>]
                        [<a href="https://videogigagan.github.io/">Webpage</a>]
                        [<a href="https://videogigagan.github.io/#BibTeX">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="200" align="center" src="https://jeanne-wang.github.io/jumpcutsmoothing/assets/method.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Jump Cut Smoothing for Talking Heads</b><br>
                        <span style="font-size: 10pt;">
                        <a href="https://jeanne-wang.github.io/">Xiaojuan Wang</a>,
                        <a href="https://taesung.me/">Taesung Park</a>, 
                        <a href="https://research.adobe.com/person/yang-zhou/">Yang Zhou</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        Richard Zhang
                        <br>
                        In ArXiv, 2024. <br>
                        [<a href="https://arxiv.org/abs/2401.04718">Paper</a>]
                        [<a href="https://jeanne-wang.github.io/jumpcutsmoothing/">Webpage</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="225" align="center" src="index_files/teaser_dreamsim.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://stephanie-fu.github.io/">Stephanie Fu</a>*, 
                        <a href="https://netanel-tamir.github.io/">Netanel Tamir</a>*, 
                        <a href="https://ssundaram21.github.io/">Shobhita Sundaram</a>*, 
                        <a href="http://people.csail.mit.edu/lrchai/">Lucy Chai</a>, 
                        Richard Zhang,
                        <a href="http://web.mit.edu/phillipi/">Tali Dekel</a>,
                        <a href="http://web.mit.edu/phillipi/">Phillip Isola</a><br>(*equal contribution)
                        <br>
                        In NeurIPS (spotlight), 2023. <br>
                        [<a href="https://arxiv.org/abs/2306.09344">Paper</a>]
                        [<a href="https://dreamsim-nights.github.io/">Webpage</a>]
                        [<a href="https://github.com/ssundaram21/dreamsim">GitHub</a>]
                        [<a href="https://github.com/ssundaram21/dreamsim#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="250" align="center" src="index_files/datattribution_teaser.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Evaluating Data Attribution for Text-to-Image Models</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://peterwang512.github.io">Sheng-Yu Wang</a>, <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, Richard Zhang<br>
                        In ICCV, 2023. <br>
                        [<a href="https://arxiv.org/abs/2306.09345">Paper</a>]
                        [<a href="https://peterwang512.github.io/GenDataAttribution/">Webpage</a>]
                        [<a href="https://github.com/peterwang512/GenDataAttribution">GitHub</a>]
                        [<a href="index_files/bibtex_arxiv23_datattribution">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="200" align="center" src="index_files/teaser_ablate.jpeg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Ablating Concepts in Text-to-Image Diffusion Models</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://nupurkmr9.github.io/">Nupur Kumari</a>, Bingliang Zhang, <a href="https://peterwang512.github.io">Sheng-Yu Wang</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, Richard Zhang, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In ICCV, 2023. <br>
                        [<a href="https://arxiv.org/abs/2303.13516">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~concept-ablation/">Webpage</a>]
                        [<a href="https://github.com/nupurkmr9/concept-ablation">GitHub</a>]
                        [<a href="https://github.com/nupurkmr9/concept-ablation#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="250" align="center" src="index_files/onlinegenai.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Online Detection of AI-Generated Images</b> <br>
                        <span style="font-size: 10pt;">
                        David C. Epstein, Ishan Jain, <a href="http://www.oliverwang.info/">Oliver Wang</a>, Richard Zhang<br>
                        In ICCV DFAD Workshop, 2023. <br>
                        [<a href="https://arxiv.org/abs/2310.15150">Paper</a>]
                        [<a href="https://richzhang.github.io/OnlineGenAIDetection">Webpage</a>]
                        [<a href="https://www.dropbox.com/scl/fi/31zlp098mmjydde42eu2f/iccv_presentation_genai_detection_FINAL.pptx?rlkey=bt7ibittpv5h6d7n208s1n6bu&dl=0">Slides</a>]
                        [<a href="https://www.dropbox.com/scl/fi/acfydxgdlxss48q3jii2d/iccv_dfad_poster.pdf?rlkey=3fzu1z5jynix3t86enurtd1w3&dl=0">Poster</a>]
                        [<a href="https://richzhang.github.io/OnlineGenAIDetection/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="180" align="center" src="https://pix2pixzero.github.io/assets/main_v6.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Zero-shot Image-to-Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://gauravparmar.com/">Gaurav Parmar</a>,
                        <a href="http://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>,
                        Richard Zhang,
                        <a href="https://yijunmaverick.github.io/">Yijun Li</a>,
                        <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In SIGGRAPH, 2023. <br>
                        [<a href="https://arxiv.org/abs/2302.03027">Paper</a>]
                        [<a href="https://pix2pixzero.github.io/">Webpage</a>]
                        [<a href="https://github.com/pix2pixzero/pix2pix-zero">GitHub</a>]
                        [<a href="https://huggingface.co/spaces/pix2pix-zero-library/pix2pix-zero-demo">Demo</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="100" align="center" src="index_files/gigagan_teaser.jpg" border="0"> &nbsp;
                        <img width="126" align="center" src="index_files/gigagan_teaser2.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Scaling up GANs for Text-to-Image Synthesis</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://mingukkang.github.io/">Minguk Kang</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, Richard Zhang, <a href="https://jaesik.info/">Jaesik Park</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="https://research.adobe.com/person/sylvain-paris/">Sylvain Paris</a>, <a href="https://taesung.me/">Taesung Park</a><br>
                        In CVPR (highlight), 2023. <br>
                        [<a href="https://arxiv.org/abs/2303.05511">Paper</a>]
                        [<a href="https://mingukkang.github.io/GigaGAN/">Webpage</a>]
                        [<a href="https://mingukkang.github.io/GigaGAN/#bibtex">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="250" align="center" src="index_files/custom_diffusion_teaser.png" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Multi-Concept Customization of Text-to-Image Diffusion</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://nupurkmr9.github.io/">Nupur Kumari</a>, Bingliang Zhang, Richard Zhang, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In CVPR, 2023. <br>
                        [<a href="https://arxiv.org/abs/2212.04488">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~custom-diffusion/">Webpage</a>]
                        [<a href="https://github.com/adobe-research/custom-diffusion">GitHub</a>]
                        [<a href="https://github.com/adobe-research/custom-diffusion#references">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <video width="200" class="video lazy" autoplay loop playsinline controls muted>
                            <source src="https://yotamnitzan.github.io/domain-expansion/assets/videos/teaser_video.mp4" type="video/mp4"></source>
                        </video>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Domain Expansion of Image Generators</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://yotamnitzan.github.io/">Yotam Nitzan</a>, <a href="http://www.mgharbi.com/">MichaÃ«l Gharbi</a>, Richard Zhang, <a href="https://taesung.me/">Taesung Park</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="https://danielcohenor.com/">Daniel Cohen-Or</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a><br>
                        In CVPR, 2023. <br>
                        [<a href="https://arxiv.org/abs/2301.05225">Paper</a>]
                        [<a href="https://yotamnitzan.github.io/domain-expansion/">Webpage</a>]
                        [<a href="https://github.com/adobe-research/domain-expansion">GitHub</a>]
                        [<a href="https://github.com/adobe-research/domain-expansion#bibtex">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="230" align="center" src="index_files/arxiv21_expandcollapse_teaser.jpg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>The Low-Rank Simplicity Bias in Deep Networks</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/minhuh/">Minyoung Huh</a>, <a href="https://people.csail.mit.edu/hmobahi/">Hossein Mobahi</a>, Richard Zhang, <a href="https://scholar.google.com/citations?user=7N-ethYAAAAJ&hl=en">Brian Cheung</a>, <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a><br>
                        In TMLR, 2023. <br>
                        [<a href="https://arxiv.org/abs/2103.10427">Paper</a>]
                        [<a href="https://minyoungg.github.io/overparam">Webpage</a>]
                        [<a href="https://github.com/minyoungg/overparam">GitHub</a>]
                        [<a href="https://github.com/minyoungg/overparam#3-cite">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="100" align="center" src="https://github.com/chail/anyres-gan/blob/main/img/github_loop.gif?raw=true" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Any-resolution Training for High-resolution Image Synthesis</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/lrchai/">Lucy Chai</a>, <a href="http://www.mgharbi.com/">MichaÃ«l Gharbi</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, 
                        <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, Richard Zhang<br>
                        In ECCV, 2022. <br>
                        [<a href="https://arxiv.org/abs/2204.07156">Paper</a>]
                        [<a href="https://chail.github.io/anyres-gan/">Webpage</a>]
                        [<a href="https://github.com/chail/anyres-gan">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=0l11u2HzQrQ&feature=emb_title&ab_channel=LucyChai">Video</a>]
                        [<a href="https://chail.github.io/anyres-gan/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <!-- <img width="100" align="center" src="https://dave.ml/blobgan/static/vids/moveall_bg.mp4" border="0"> &nbsp; -->
                        <video src="https://dave.ml/blobgan/static/vids/moveall_bg.mp4" data-no-pause="" autoplay="" playsinline="" muted="" loop="" width=180></video>
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>BlobGAN: Spatially Disentangled Scene Representations</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://dave.ml/">Dave Epstein</a>, <a href="https://taesung.me/">Taesung Park</a>, Richard Zhang, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a><br>
                        In ECCV, 2022. <br>
                        [<a href="https://arxiv.org/abs/2205.02837">Paper</a>]
                        [<a href="https://dave.ml/blobgan/">Webpage</a>]
                        [<a href="https://github.com/dave-epstein/blobgan">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=KpUv82VsU5k">Video</a>]
                        [<a href="https://dave.ml/blobgan/#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="200" align="center" src="https://lychenyoko.github.io/3D-FM-GAN-Webpage/resources/Teaser.gif?raw=true" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>3D-FM GAN: Towards 3D-Controllable Face Manipulation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://lychenyoko.github.io/">Yuchen Liu</a>,
                        <a href="https://zhixinshu.github.io/">Zhixin Shu</a>,
                        <a href="https://yijunmaverick.github.io/">Yijun Li</a>,
                        <a href="https://sites.google.com/site/zhelin625/">Zhe Lin</a>,
                        Richard Zhang,
                        <a href="https://ece.princeton.edu/people/sun-yuan-kung">Sun-Yuan Kung</a> <br>
                        In ECCV, 2022. <br>
                        [<a href="https://arxiv.org/abs/2208.11257">Paper</a>]
                        [<a href="https://lychenyoko.github.io/3D-FM-GAN-Webpage/">Webpage</a>]
                        <!-- [<a href="">GitHub</a>] -->
                        [<a href="https://www.youtube.com/watch?v=3tR7qIXyzLE">Video</a>]
                        <!-- [<a href="">Bibtex</a>] -->
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="180" align="center" src="https://people.cs.umass.edu/~dliu/df_files/sig22.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>ASSET: Autoregressive Semantic Scene Editing with Transformers at High Resolutions</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://people.cs.umass.edu/~dliu/">Difan Liu</a>, Sandesh Shetty, <a href="http://www.tobiashinz.com/">Tobias Hinz</a>, <a href="https://techmatt.github.io/">Matthew Fisher</a>, Richard Zhang, <a href="https://taesung.me/">Taesung Park</a>, <a href="https://people.cs.umass.edu/~kalo/">Evangelos Kalogerakis</a><br>
                        In SIGGRAPH, 2022. <br>
                        [<a href="https://arxiv.org/abs/2205.12231">Paper</a>]
                        [<a href="https://people.cs.umass.edu/~dliu/projects/ASSET/">Webpage</a>]
                        [<a href="https://github.com/DifanLiu/ASSET">GitHub</a>]
                        [<a href="https://people.cs.umass.edu/~dliu/projects/ASSET/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=center>
                        <img width="230" align="center" src="index_files/cats_cube_3.gif" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>GAN-Supervised Dense Visual Alignment</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.wpeebles.com/">William Peebles</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                        Richard Zhang,
                        <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
                        <a href="https://groups.csail.mit.edu/vision/torralbalab/">Antonio Torralba</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a><br>
                        In CVPR (oral, best paper finalist), 2022. <br>
                        [<a href="https://arxiv.org/abs/2112.05143">Paper</a>]
                        [<a href="https://www.wpeebles.com/gangealing.html">Webpage</a>]
                        [<a href="https://github.com/wpeebles/gangealing">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=Qa1ASS_NuzE">Video</a>]
                        [<a href="https://www.wpeebles.com/gangealing_bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                        <img width="180" align="center" src="index_files/arxiv2021_visionaid_teaser.jpg" border="0"> &nbsp;
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Ensembling Off-the-shelf Models for GAN Training</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://nupurkmr9.github.io/">Nupur Kumari</a>,
                        Richard Zhang,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In CVPR (oral), 2022. <br>
                        [<a href="https://arxiv.org/abs/2112.09130">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~vision-aided-gan/">Webpage</a>]
                        [<a href="https://github.com/nupurkmr9/vision-aided-gan">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=oHdyJNdQ9E4">Video</a>]
                        [<a href="index_files/bibtex_arxiv2021_visionaid.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="240" align="center" src="https://gauravparmar.com/images/teaser_prepped_website.jpg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Spatially-Adaptive Multilayer Selection for GAN Inversion and Editing</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://gauravparmar.com/">Gaurav Parmar</a>,
                        <a href="https://yijunmaverick.github.io/">Yijun Li</a>,
                        <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>,
                        Richard Zhang,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                        <a href="http://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a>
                        <br>
                        In CVPR, 2022. <br>
                        [<a href="https://arxiv.org/abs/2206.08357">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~SAMInversion/">Webpage</a>]
                        [<a href="https://github.com/adobe-research/sam_inversion">GitHub</a>]
                        [<a href="https://www.cs.cmu.edu/~SAMInversion/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="https://raw.githubusercontent.com/GaParmar/clean-fid/main/docs/images/resize_circle.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>On Aliased Resizing Libraries and Surprising Subtleties in FID Calculation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://gauravparmar.com/">Gaurav Parmar</a>, Richard Zhang, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In CVPR, 2022. <br>
                        [<a href="https://arxiv.org/abs/2104.11222">Paper</a>]
                        [<a href="https://www.cs.cmu.edu/~clean-fid/">Webpage</a>]
                        [<a href="https://github.com/GaParmar/clean-fid">GitHub</a>]
                        [<a href="https://github.com/GaParmar/clean-fid#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="index_files/editnerf.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Editing Conditional Radiance Fields</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/stevenliu/">Steven Liu</a>, <a href="https://people.csail.mit.edu/xiuming/">Xiuming Zhang</a>, <a href="https://ztzhang.info/">Zhoutong Zhang</a>, Richard Zhang, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="http://bryanrussell.org/">Bryan Russell</a><br>
                        In ICCV, 2021. <br>
                        [<a href="https://arxiv.org/abs/2105.06466">Paper</a>]
                        [<a href="http://editnerf.csail.mit.edu/">Webpage</a>]
                        [<a href="https://github.com/stevliu/editnerf">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=9qwRD4ejOpw">Video</a>]
                        [<a href="https://colab.research.google.com/github/stevliu/editnerf/blob/master/editnerf.ipynb">Demo</a>]
                        [<a href="https://github.com/stevliu/editnerf#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="200" align="center" src="index_files/cfl_teaser.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Contrastive Feature Loss for Image Prediction</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.alexandonian.com/">Alex Andonian</a>,
                        <a href="https://taesung.me/">Taesung Park</a>,
                        <a href="http://bryanrussell.org/">Bryan Russell</a>,
                        <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
                        <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>,
                        Richard Zhang.<br>
                        In ICCV AIM Workshop, 2021. <br>
                        [<a href="https://arxiv.org/abs/2111.06934">Paper</a>]
                        [<a href="https://github.com/alexandonian/contrastive-feature-loss">GitHub</a>]
                        [<a href="index_files/bibtex_iccvaim21_cfl.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="180" align="center" src="https://github.com/chail/gan-ensembling/blob/main/img/teaser.gif?raw=true" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Ensembling with Deep Generative Views</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/lrchai//">Lucy Chai</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, Richard Zhang<br>
                        In CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2104.14551">Paper</a>]
                        [<a href="https://chail.github.io/gan-ensembling/">Webpage</a>]
                        [<a href="https://github.com/chail/gan-ensembling">GitHub</a>]
                        [<a href="https://www.youtube.com/channel/UCty2ywzQwRx-qpbV1S8EiKQ">Video</a>]
                        [<a href="https://colab.research.google.com/drive/1-qZBjn07KlWv27kKQGaKOXMBgP-Fb0Ws?usp=sharing">Colab</a>]
                        [<a href="https://chail.github.io/gan-ensembling/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="https://github.com/utkarshojha/few-shot-gan-adaptation/blob/gh-pages/resources/concept.gif?raw=true" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Few-shot Image Generation via Cross-domain Correspondence</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://utkarshojha.github.io/">Utkarsh Ojha</a>, <a href="https://yijunmaverick.github.io/">Yijun Li</a>, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>, <a href="https://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, <a href="https://web.cs.ucdavis.edu/~yjlee/">Yong Jae Lee</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, Richard Zhang<br>
                        In CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2104.06820">Paper</a>]
                        [<a href="https://utkarshojha.github.io/few-shot-gan-adaptation/">Webpage</a>]
                        [<a href="https://github.com/utkarshojha/few-shot-gan-adaptation">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=GCDm8hOlpHs">Video</a>]
                        [<a href="https://utkarshojha.github.io/few-shot-gan-adaptation/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="https://hanlab18.mit.edu/projects/anycost-gan/images/flexible.gif" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Anycost GANs for Interactive Image Synthesis and Editing</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://linji.me/">Ji Lin</a>, Richard Zhang, Frieder Ganz, <a href="https://songhan.mit.edu/">Song Han</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2103.03243">Paper</a>]
                        [<a href="https://hanlab18.mit.edu/projects/anycost-gan/">Webpage</a>]
                        [<a href="https://www.youtube.com/watch?v=_yEziPl9AkM">Video</a>]
                        [<a href="https://github.com/mit-han-lab/anycost-gan">GitHub</a>]
                        [<a href="https://github.com/mit-han-lab/anycost-gan#citation">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img height="90" align="center" src="index_files/asapnet_teaser.png" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Spatially-Adaptive Pixelwise Networks for Fast Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://stamarot.webgr.technion.ac.il/">Tamar Rott Shaham</a>, <a href="http://www.mgharbi.com/">MichaÃ«l Gharbi</a>, Richard Zhang, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="https://tomer.net.technion.ac.il/">Tomer Michaeli</a><br>
                        In CVPR, 2021. <br>
                        [<a href="https://arxiv.org/abs/2012.02992">Paper</a>]
                        [<a href="https://tamarott.github.io/ASAPNet_web/">Webpage</a>]
                        [<a href="https://tamarott.github.io/ASAPNet_web/resources/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="220" align="center" src="./index_files/cdpam_teaser.jpg" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>CDPAM: Contrastive Learning for Perceptual Audio Similarity</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.cs.princeton.edu/~pmanocha/">Pranay Manocha</a>, <a href="https://research.adobe.com/person/zeyu-jin/">Zeyu Jin</a>, Richard Zhang, <a href="https://www.cs.princeton.edu/~af/">Adam Finkelstein</a><br>
                        In ICASSP, 2021. <br>
                        [<a href="https://arxiv.org/abs/2102.05109">Paper</a>]
                        [<a href="https://pixl.cs.princeton.edu/pubs/Manocha_2021_CCL/index.php">Webpage</a>]
                        [<a href="https://github.com/pranaymanocha/PerceptualAudio">GitHub</a>]
                        [<a href="./index_files/bibtex_icassp2021_audio.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img height="80" align="center" src="https://taesung.me/SwappingAutoencoder/index_files/church_style_swaps.gif" border="0"> &nbsp;
                        <img height="80" align="center" src="https://taesung.me/SwappingAutoencoder/index_files/tree_smaller.gif" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Swapping Autoencoder for Deep Image Manipulation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://taesung.me/">Taesung Park</a>, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, Richard Zhang<br>
                        In NeurIPS, 2020. <br>
                        [<a href="https://arxiv.org/abs/2007.00653">Paper</a>]
                        [<a href="https://taesung.me/SwappingAutoencoder">Webpage</a>]
                        [<a href="https://github.com/taesungp/swapping-autoencoder-pytorch">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=0elW11wRNpg&feature=emb_title">Video</a>]
                        [<a href="https://taesung.me/SwappingAutoencoder/index_files/bibtex_arxiv2020.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img height="100" align="center" src="./index_files/fewshot_neurips20.jpg" border="0"> &nbsp;
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Few-shot Image Generation with Elastic Weight Consolidation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://yijunmaverick.github.io/">Yijun Li</a>, Richard Zhang, <a href="https://research.adobe.com/person/jingwan-lu/">Jingwan Lu</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a><br>
                        In NeurIPS, 2020. <br>
                        [<a href="https://arxiv.org/abs/2012.02780">Paper</a>]
                        [<a href="https://proceedings.neurips.cc/paper/2020/file/b6d767d2f8ed5d21a44b0e5886680cb9-Supplemental.pdf">Supplemental</a>]
                        [<a href="https://yijunmaverick.github.io/publications/ewc/">Webpage</a>]
                        [<a href="./index_files/bibtex_fewshot_neurips20.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img height="100" align="center" src="index_files/eccv2020_cut.jpg" border="0"> &nbsp;
                        <!-- <img height="110" align="center" src="index_files/eccv2020_cut.jpg" border="0"> &nbsp; -->
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Contrastive Learning for Unpaired Image-to-Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://taesung.me/">Taesung Park</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, Richard Zhang, <a href="https://www.cs.cmu.edu/~junyanz/">Jun-Yan Zhu</a><br>
                        In ECCV, 2020. <br>
                        [<a href="https://arxiv.org/abs/2007.15651">Paper</a>]
                        [<a href="http://taesung.me/ContrastiveUnpairedTranslation">Webpage</a>]
                        [<a href="https://github.com/taesungp/contrastive-unpaired-translation">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=Llg0vE_MVgk&feature=emb_title">Teaser</a>]
                        [<a href="https://www.youtube.com/watch?v=jSGOzjmN8q0">Video</a>]
                        [<a href="http://taesung.me/ContrastiveUnpairedTranslation/index_files/bibtex_eccv2020.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=left>
                        <img width="260" align="center" src="./index_files/arxiv2020_project_teaser.jpg" border="0">
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> Transforming and Projecting Images into Class-conditional Generative Networks</b><br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/minhuh/">Minyoung Huh</a>, Richard Zhang, <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>, <a href="http://people.csail.mit.edu/sparis/">Sylvain Paris</a>, <a href="https://www.dgp.toronto.edu/~hertzman/">Aaron Hertzmann</a><br>
                        In ECCV (oral), 2020. <br>
                        <!-- [<a href="">Paper</a>] -->
                        [<a href="https://arxiv.org/abs/2005.01703">Paper</a>]
                        [<a href="https://minyoungg.github.io/pix2latent/">Webpage</a>]
                        [<a href="https://github.com/minyoungg/pix2latent">GitHub</a>]
                        [<a href="./index_files/bibtex_arxiv2020_trans_proj.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="260" align="center" src="./index_files/audio_teaser.jpg" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.cs.princeton.edu/~pmanocha/">Pranay Manocha</a>, <a href="https://www.cs.princeton.edu/~af/">Adam Finkelstein</a>, Richard Zhang, <a href="https://ccrma.stanford.edu/~njb/">Nicholas J. Bryan</a>, <a href="https://ccrma.stanford.edu/~gautham/Site/Gautham_J._Mysore.html">Gautham J. Mysore</a>, <a href="https://research.adobe.com/person/zeyu-jin/">Zeyu Jin</a><br>
                        In Interspeech, 2020. <br>
                        [<a href="https://arxiv.org/abs/2001.04460">Paper</a>]
                        [<a href="https://gfx.cs.princeton.edu/pubs/Manocha_2020_ADP/">Webpage</a>]
                        [<a href="https://github.com/pranaymanocha/PerceptualAudio">GitHub</a>]
                        [<a href="./index_files/bibtex_arxiv2020_audio.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=center>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="220" align="center" src="./index_files/cnndetect_teaser2.png" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>CNN-generated images are surprisingly easy to spot...for now</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://peterwang512.github.io">Sheng-Yu Wang</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, Richard Zhang, <a href="http://andrewowens.com">Andrew Owens</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a> <br>
                        In CVPR, 2020 (oral). <br>
                        [<a href="https://arxiv.org/abs/1912.11035">Paper</a>]
                        [<a href="https://peterwang512.github.io/CNNDetection/">Webpage</a>]
                        [<a href="https://github.com/PeterWang512/CNNDetection">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=98bCHTkp5sE">Talk</a>]
                        [<a href="https://peterwang512.github.io/CNNDetection/bibtex.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="250" align="center" src="./index_files/shape_2019_teaser.jpg" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Deep Parametric Shape Predictions using Distance Fields</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.csail.mit.edu/smirnov/">Dmitriy Smirnov</a>,
                        <a href="https://research.adobe.com/person/matt-fisher/">Matthew Fisher</a>,
                        <a href="https://research.adobe.com/person/vladimir-kim/">Vladimir G. Kim</a>,
                        Richard Zhang,
                        <a href="https://people.csail.mit.edu/jsolomon/">Justin Solomon</a><br>
                        In CVPR, 2020. <br>
                        [<a href="https://arxiv.org/abs/1904.08921">Paper</a>]
                        [<a href="https://people.csail.mit.edu/smirnov/deep-parametric-shapes/">Webpage</a>]
                        [<a href="https://github.com/dmsm/DeepParametricShapes">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=v_0UrjbTtHg">Video</a>]
                        [<a href="./index_files/bibtex_arxiv2019_shape.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <div style="width: 260; height: 80; margin: -10px 0px 0px 0px; overflow: hidden">
                            <img width="75" align="center" src="./index_files/morph_1028_AB.jpg_100619_AB.jpg.gif" border="0">
                            <img width="75" align="center" src="./index_files/morph_7994928.362576.jpg_7590611.3.jpg.gif" border="0">
                            <img width="100" align="center" src="./index_files/morph_3113.png_3938.png.gif" border="0">
                        </div>
                        <!-- <img width="80" align="center" src="./index_files/morph_100938_AB.jpg_27721_AB.jpg.gif" border="0"> -->
                        <!-- <img width="80" align="center" src="./index_files/morph_f58a26c915e7a1dceae7a0fa074b4a2a_1.png_7805239ad1e40e07c69d7040c52664c5_1.png.gif" border="0"> -->
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b> Image Morphing with Perceptual Constraints and STN Alignment </b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://www.cs.tau.ac.il/~noafish/">Noa Fish</a>, Richard Zhang, Lilach Perry, <a href="https://www.cs.tau.ac.il/~dcor/index.html">Daniel Cohen-Or</a>, <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="http://www.connellybarnes.com/">Connelly Barnes</a><br>
                        In CGF, 2020. <br>
                        [<a href="https://arxiv.org/abs/2004.14071">Paper</a>]
                        [<a href="https://github.com/noafish/MorphGAN">GitHub</a>]
                        [<a href="./index_files/bibtex_arxiv2020_morph.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2019</b></span>
                </td></tr> -->
                
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="125" align="center" src="./index_files/aacnns_2019_teaser.gif" border="0">
                        <img width="125" align="center" src="./index_files/aacnns_2019_teaser2.gif" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Making Convolutional Networks Shift-Invariant Again</b> <br>
                        <span style="font-size: 10pt;">
                        Richard Zhang <br>
                        In ICML, 2019. <br>
                        <!-- [<a href="https://richzhang.github.io/antialiased-cnns/resources/camera-ready.pdf">Paper</a>] -->
                        [<a href="https://arxiv.org/abs/1904.11486">Paper</a>]
                        [<a href="https://richzhang.github.io/antialiased-cnns/">Webpage</a>]
                        [<a href="https://github.com/adobe/antialiased-cnns">GitHub</a>]
                        [<a href='https://www.youtube.com/watch?v=HjewNBZz00w&feature=youtu.be'>Talk</a>]
                        [<a href='https://www.dropbox.com/s/4mco6s76d9hi00n/antialiasing_cnns.pptx?dl=0'>Slides</a> (129mb)]
                        [<a href="https://www.dropbox.com/s/dhf2gqt14sq76q7/poster_icml.pdf?dl=0">Poster</a>]
                        [<a href="./index_files/bibtex_icml2019.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <!-- <center> -->
                        <img width="250" align="center" src="./index_files/falteaser.png" border="0">
                        <!-- </center> -->
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Detecting Photoshopped Faces by Scripting Photoshop</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://peterwang512.github.io/">Sheng-Yu Wang</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, <a href="http://andrewowens.com">Andrew Owens</a>, Richard Zhang, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a> <br>
                        In ICCV, 2019. <br>
                        [<a href="https://arxiv.org/abs/1906.05856">Paper</a>]
                        [<a href="https://peterwang512.github.io/FALdetector/">Webpage</a>]
                        [<a href="https://github.com/peterwang512/FALdetector">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=TUootD36Xm0">Video</a>]
                        [<a href="https://www.dropbox.com/s/qn4c19134zjziyi/%5BFinal%5D%20ICCV%20Poster.pdf?dl=0">Poster</a>]
                        [<a href="https://www.youtube.com/watch?v=21lj8tCSMkg">Adobe Max</a>]
                        [<a href="https://business.adobe.com/blog/the-latest/adobe-research-and-uc-berkeley-detecting-facial-manipulations-in-adobe-photoshop">Adobe Blog</a>]
                        [<a href="./index_files/bibtex_fal2019.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>

                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="125" align="center" src="./index_files/isketchnfill_teaser.gif" border="0">
                        <img width="125" align="center" src="./index_files/isketchnfill_teaser2.gif" border="0">
                    </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Interactive Sketch & Fill: Multiclass Sketch-to-Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="https://arnabgho.github.io/">Arnab Ghosh</a>,
                        Richard Zhang,
                        <a href="https://puneetkdokania.github.io/">Puneet Dokania</a>,
                        <a href="http://www.oliverwang.info/">Oliver Wang</a>,
                        <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>,
                        <a href="http://www.robots.ox.ac.uk/~phst/">Philip H.S. Torr</a>,
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>
                        In ICCV, 2019. <br>
                        [<a href="https://arxiv.org/abs/1909.11081">Paper</a>]
                        [<a href="https://arnabgho.github.io/iSketchNFill/">Webpage</a>]
                        [<a href="https://github.com/arnabgho/iSketchNFill">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=T9xtpAMUDps">Video</a>]
                        [<a href="./index_files/bibtex_iccv2019_isketchnfill.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2018</b></span>
                </td></tr> -->
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="250" align="center" src="./index_files/perceptual_teaser.jpg" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>The Unreasonable Effectiveness of Deep Features as a Perceptual Metric</b> <br>
                        <span style="font-size: 10pt;">
                        Richard Zhang, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a><br>
                        In CVPR, 2018. <br>
                        [<a href="http://arxiv.org/abs/1801.03924">Paper</a>]
                        [<a href="https://richzhang.github.io/PerceptualSimilarity/">Webpage</a>]
                        [<a href="https://github.com/richzhang/PerceptualSimilarity">GitHub</a>]
                        [<a href="https://richzhang.github.io/PerceptualSimilarity/index_files/poster_cvpr.pdf">Poster</a>]
                        [<a href="https://research.adobe.com/with-deep-learning-computers-see-images-more-like-humans-do/">Adobe Blog</a>]
                        [<a href="https://www.youtube.com/watch?v=DglrYx9F3UU">Two Min Papers</a>]
                        [<a href="./index_files/bibtex_cvpr2018.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="250" align="center" src="./index_files/savp3.gif" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Stochastic Adversarial Video Prediction</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.eecs.berkeley.edu/~alexlee_gk/index.html">Alex X. Lee</a>, Richard Zhang, 
                        <a href="https://febert.github.io/">Frederik Ebert</a>, <a href="http://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>, <a href="https://people.eecs.berkeley.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a> <br>
                        In ArXiv, 2018. <br>
                        [<a href="https://arxiv.org/abs/1804.01523">Paper</a>]
                        [<a href="https://alexlee-gk.github.io/video_prediction/">Webpage</a>]
                        [<a href="https://github.com/alexlee-gk/video_prediction">GitHub</a>]
                        [<a href="./index_files/bibtex_savp.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2017</b></span>
                </td></tr> -->
                
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <img width="250" align="center" src="./index_files/nips2017.jpg" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Toward Multimodal Image-to-Image Translation</b> <br>
                        <span style="font-size: 10pt;">
                        <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>, Richard Zhang, <a href="http://people.eecs.berkeley.edu/~pathak/">Deepak Pathak</a>, <a href="https://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a>, <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, <a href="http://www.oliverwang.info/">Oliver Wang</a>, 
                        <a href="https://research.adobe.com/person/eli-shechtman/">Eli Shechtman</a> <br>
                        In NIPS, 2017. <br>
                        [<a href="https://arxiv.org/abs/1711.11586">Paper</a>]
                        <!-- <a href="https://papers.nips.cc/paper/6650-toward-multimodal-image-to-image-translation">Official</a>] -->
                        [<a href="https://junyanz.github.io/BicycleGAN/">Webpage</a>]
                        [<a href="https://github.com/junyanz/BicycleGAN">GitHub</a>]
                        [Video (<a href="https://www.youtube.com/watch?v=JvGysD2EFhw">YouTube</a>)(<a href="http://efrosgans.eecs.berkeley.edu/BicycleGAN/video_extended.mp4">mp4</a>)]
                        [<a href="http://junyanz.github.io/BicycleGAN/index_files/poster_nips_v3.pdf">Poster</a>]
                        [<a href="https://www.youtube.com/watch?v=XcxzKLrCpyk">Two Min Papers</a>]
                        [<a href="./index_files/bibtex_nips2017.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                    <!-- height="74" -->
                        <a href="http://richzhang.github.io/ideepcolor"><img width="250" align="center" src="./index_files/siggraph2017_update.jpg" border="0"></a>
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Real-Time User-Guided Image Colorization with Learned Deep Priors</b> <br>
                        <span style="font-size: 10pt;">
                        Richard Zhang*, <a href="http://people.eecs.berkeley.edu/~junyanz/">Jun-Yan Zhu</a>*, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>, <a href="http://young-geng.xyz/">Xinyang Geng</a>, <a href="http://www.cs.utexas.edu/~alin/">Angela S. Lin</a>, <a href="https://tianheyu927.github.io/">Tianhe Yu</a>, 
                        <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>        <br>
                        (*equal contribution) <br>
                        In SIGGRAPH, 2017. <br>
                        [<a href="https://arxiv.org/abs/1705.02999">Paper</a>]
                        <!-- <a href="https://dl.acm.org/citation.cfm?id=3073703">Official</a>] -->
                        [<a href="https://richzhang.github.io/InteractiveColorization/">Webpage</a>]
                        [<a href="https://youtu.be/eiFzQI7LzO0?t=5690">Fastforward</a>]
                        [<a href="https://www.youtube.com/watch?v=rp5LUSbdsys">Talk</a>]
                        [Video (<a href="https://www.youtube.com/watch?v=eL5ilZgM89Q&feature=youtu.be">YouTube</a>)(<a href="https://www.dropbox.com/s/mfi66auuv7qzyx0/iColor_release.mp4?dl=0">mp4</a>)]
                        [<a href="http://video.tv.adobe.com/v/28291">PSE 2020</a>]
                        [<a href="https://github.com/junyanz/interactive-deep-colorization">GitHub</a>]
                        [<a href="https://www.dropbox.com/s/urmifx558nw0ogi/release.pptx?dl=0">Slides</a> (141mb)]
                        [<a href="./index_files/bibtex_siggraph2017.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
                <tr>
                    <td width="30%" align=left>
                        <a href="http://richzhang.github.io/splitbrainauto"><img width="250" align="center" src="./index_files/cvpr2017_splitbrain.png" border="0"></a>
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Split-Brain Autoencoders: Unsupervised Learning by Cross-Channel Prediction</b><br>
                        <span style="font-size: 10pt;">
                        Richard Zhang, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
                        <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>        <br>
                        In CVPR, 2017.
                        <br>
                        [<a href="https://arxiv.org/abs/1611.09842">Paper</a>]
                        <!-- (<a href="http://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Split-Brain_Autoencoders_Unsupervised_CVPR_2017_paper.html">official</a>)] -->
                        [<a href="http://richzhang.github.io/splitbrainauto">Webpage</a>]
                        [<a href="https://github.com/richzhang/splitbrainauto">GitHub</a>]
                        [<a href="https://richzhang.github.io/splitbrainauto/index_files/poster_cvpr.pdf">Poster</a>]
                        [<a href="https://www.youtube.com/watch?v=FTzcFsz2xqw">Seminar Talk</a>]
                        [<a href="./index_files/bibtex_cvpr2017_splitbrain.txt">Bibtex</a>]</span>
                    </td>
                </tr>


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2016</b></span>
                </td></tr> -->
                
                <tr>
                    <td width="30%" align=left>
                    <!-- height="90" -->
                        <a href="http://richzhang.github.io/colorization/"><img width="250" align="center" src="./index_files/arxiv2016_colorization.jpg" border="0"></a>
                        </td>
                        <td>
                        <span style="font-size: 12pt;"><b>Colorful Image Colorization</span></b><br>
                        <span style="font-size: 10pt;">
                        Richard Zhang, <a href="http://web.mit.edu/phillipi/">Phillip Isola</a>,
                        <a href="http://www.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>        <br>
                        In ECCV, 2016 (oral).
                        <br>
                        [<a href="https://arxiv.org/abs/1603.08511">Paper</a>]
                        <!-- <a href="https://link.springer.com/chapter/10.1007/978-3-319-46487-9_40">Official</a>] -->
                        [<a href="http://richzhang.github.io/colorization/">Webpage</a>]
                        [<a href="https://github.com/richzhang/colorization">GitHub</a>]
                        [<a href="https://www.youtube.com/watch?v=4xoTD58Wt-0">Talk</a>]
                        [<a href="https://www.dropbox.com/s/sa8m3y1ymj0ihct/presentation_eccv_release.pptx?dl=0">Slides</a> (138mb)]
                        [<a href="http://www.eccv2016.org/files/posters/O-2B-03.pdf">Poster</a>]
                        [<a href="./index_files/bibtex_eccv2016_colorization.txt">Bibtex</a>]
                    </td>
                </tr>

<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2015</b></span>
                </td></tr> -->
                
                <tr>
					<td width="30%" align=left>
						<img width="250" align="center" src="./index_files/icra2015.png" border="0">
                    </td>
					<td>
                        <span style="font-size: 12pt;">
                        <b>Sensor Fusion for Semantic Segmentation of Urban Scenes</b><br>
                        <span style="font-size: 10pt;"> 
						Richard Zhang, <a href="https://www.linkedin.com/in/candrastefan/">Stefan Candra</a>, </a><a href="http://anp.lbl.gov/kai-vetter/">Kai Vetter</a>,
                        <a href="http://www-video.eecs.berkeley.edu/~avz/">Avideh Zakhor</a>		<br>
						In ICRA, 2015.
                        <br>
                        [Paper (<a href="./index_files/icra2015.pdf">pdf</a>)(<a href="http://ieeexplore.ieee.org/document/7139439/">official</a>)]

                        [<a href="./index_files/pres_icra2015.pdf">Slides</a>]
                        [<a href="./index_files/poster_icra2015.pdf">Poster</a>]
                        [<a href="https://www.youtube.com/watch?v=s9X63Ma3M0Y">Talk</a>]

                        [Annotations (<a href="http://www.eecs.berkeley.edu/~rich.zhang/projects/2015_ICRA_semanticsegmentation/KITTI_public.tar">tar</a>)(<a href="http://www.eecs.berkeley.edu/~rich.zhang/projects/2015_ICRA_semanticsegmentation/KITTI_public.zip">zip</a>) ]
                        [<a href="./index_files/bibtex_icra2015.txt">Bibtex</a>]
                        </span>
					</td>
				</tr>  


<!--                 <tr><td width="30%" align=left>
                <span style="font-size: 16pt;"><b>2014</b></span>
                </td></tr> -->
                

                <tr>
                    <td width="30%" align="center">
                    <!-- height="100" -->
                        <img height="95" horizontal-align="center" src="./index_files/wacv2014.png" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;"><b>Automatic Identification of Window Regions on Indoor Point Clouds Using LiDAR and Cameras</b><br>
                        <span style="font-size: 10pt;">
                        Richard Zhang,
                        <a href="http://www-video.eecs.berkeley.edu/~avz/">Avideh Zakhor</a>
                        <br>
                        In WACV, 2014.
                        <br>
                        [Paper (<a href="./index_files/wacv2014.pdf">pdf</a>)(<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.649.303&rep=rep1&type=pdf">official</a>)]

                        [<a href="./index_files/bibtex_wacv2014.txt">Bibtex</a>]
                        </span>
                    </td>
                </tr>

            </tbody></table>

        <h2>Thesis </h2>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <table cellspacing="15">
                <tbody>
                <tr>
                    <td width="30%" align=left>
                        <img width="250" align="center" src="./index_files/berkeley_logo.png" border="0">
                            </td>
                    <td>
                        <span style="font-size: 12pt;">
                        <b>Image Synthesis for Self-Supervised Visual Representation Learning</b> <br>
                        <span style="font-size: 10pt;">
                        Richard Zhang<br>
                        <!-- Commitee: Alexei A. Efros, Trevor Darrell, Michael DeWeese.<br> -->
                        Spring 2018.<br>
                        [<a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-36.html">Thesis</a>]
                        [<a href="https://www.youtube.com/watch?v=aGhYitrOJRc">Dissertation Talk</a>]
                        [<a href="https://youtu.be/IXG-uWFAkmM">Fast Forward</a>]
                        [<a href="https://www.dropbox.com/s/96f0xhfwvjnbf52/presentation_dissertation.pptx?dl=0">Slides</a> (396 MB)]
                        [<a href="./index_files/bibtex_thesis.txt">Bibtex</a>]
                        <br>
                    </td>
                </tr>
            </tbody></table>


        <h2>Organization, Committees </h2>
        <span style="font-size: 10pt;">
        CVPR 2020, 2021, 2023, 2024, 2025 (Area Chair)<br>
        ECCV 2024 (Area Chair)<br>
        BMVC 2022 (Area Chair)<br>
        <a href="https://she-workshop.github.io/">Sketching for Human Expressivity (SHE)</a> at ECCV 2022 (co-organizer)<br>
        <a href="https://data.vision.ee.ethz.ch/cvl/aim19/">Advances in Image Manipulation (AIM)<a> at ICCV 2019 (co-organizer)<br>

        <h2>Awards </h2>
        <span style="font-size: 10pt;">
        MIT Technology Review, <a href="https://www.technologyreview.com/innovator/richard-zhang/">35 Innovators Under 35</a>, 2023<br>
        Reviewer recognitions, CVPR 2019, NeurIPS 2019, ECCV 2020, NeurIPS 2020, ECCV 2022<br>
        Thesis Fast Forward, Best Presentation, SIGGRAPH 2018<br>
        <a href="https://research.adobe.com/fellowship/">Adobe Research Fellowship</a> 2017<br>

<!--         <h2>Tech Transfers </h2>
        <span style="font-size: 10pt;">
        Colorize Photo, Photoshop Elements 2020<br>
        Colorize, Photoshop Neural Filters 2020, 2021<br>
        Landscape Mixer, Photoshop Neural Filters 2021<br>
        Smart Portrait, Photoshop Neural Filters 2021<br>
        </span> -->

        <h2>Student collaborators/interns</h2>
        I have gotten to work with some wonderful collaborators.<br>

        <!-- <h3>Internship </h3> -->
<!--         <br>
        <font face="helvetica, ariel, &#39;sans serif&#39;">
            <span style="font-size: 10pt;">
            If you have similar interests and are interested in collaborating during a summer 2021 internship, please feel free to contact me! Tell me about your past research experience and what you would potentially like to do. The goal of an internship is a publication, usually CVPR or SIGGRAPH. For example, see my NIPS 2017 and CVPR 2018 papers, which were from my summer 2017 internship. Interns are almost all PhD students; the number of slots is limited, so we unfortunately cannot accept everyone.<br>
            </span>
        </font> -->

        <!-- <h3>@Adobe</h3> -->
        <br>
         <span style="font-size: 14pt;">
            <b>@Adobe</b>
        </span>

        <span style="font-size: 10pt;">
        <!-- <p> -->
        <dl class="dl-horizontal">
            <dt><b>PhD/MS [interns]</b></dt>
            <a href="https://graceduansu.github.io/">Grace Su</a>, CMU<br>
            <a href="https://danielchyeh.github.io/">Chun-Hsiao (Daniel) Yeh</a>, Berkeley<br>
            <a href="https://konpat.notion.site/">Konpat Preechakul</a>, Berkeley<br>
            <a href="https://ryanpo.com/">Po Ryan</a>, Stanford<br>
            <a href="https://rohitgandikota.github.io/">Rohit Gandikota</a>, CMU<br>
            <a href="https://tianweiy.github.io/">Tianwei Yin</a>, MIT<br>
            <a href="https://joaanna.github.io/">Joanna Materzynska</a>, MIT<br>
            <a href="https://twizwei.github.io/">Yiran Xu</a>, UMaryland<br>
            <a href="https://alii-ganjj.github.io/">Alireza Ganjdanesh</a>, UMaryland<br>
            <a href="https://jitengmu.github.io/">Jiteng Mu</a>, UC San Diego<br>
            <a href="http://peterwang512.github.io/">Sheng-Yu Wang</a>, CMU <br>
            <a href="https://jeanne-wang.github.io/">Xiaojuan Wang</a>, UWashington <br>
            <a href="https://yossigandelsman.github.io/">Yossi Gandelsman</a>, UC Berkeley <br>
            <a href="https://yinboc.github.io/">Yinbo Chen</a>, UC San Diego<br>
            <a href="https://nupurkmr9.github.io//">Nupur Kumari</a>, CMU<br>
            <a href="https://mingukkang.github.io/">Minguk Kang</a>, POSTTECH<<br>
            <a href="https://dave.ml/">Dave Epstein</a>, UC Berkeley<br>
            <a href="https://gauravparmar.com/">Gaurav Parmar</a>, CMU<br>
            <a href="https://yotamnitzan.github.io/">Yotam Nitzan</a>, Tel Aviv University<br>
            <a href="https://homes.cs.washington.edu/~royorel/">Roy Or-El</a>, UW<br>
            <a href="http://people.csail.mit.edu/lrchai/">Lucy Chai</a>, MIT (Fellowship winner, 2021)<br>
            <a href="https://lychenyoko.github.io/">Yuchen Liu</a>, Princeton<br>
            <a href="https://people.cs.umass.edu/~dliu/">Difan Liu</a>, UMass Amherst<br>
            <a href="https://taesung.me/">Taesung Park</a>, UC Berkeley (Fellowship winner, 2020)<br>
            <a href="http://linji.me/">Ji Lin</a>, MIT<br>
            <a href="https://www.wpeebles.com/">William (Bill) Peebles</a>, UC Berkeley<br>
            <a href="https://www.alexandonian.com/">Alex Andonian</a>, MIT<br>
            <a href="https://utkarshojha.github.io/">Utkarsh Ojha</a>, UC Davis<br>
            <a href="http://arnabgho.github.io/">Arnab Ghosh</a>, Oxford<br>
            <a href="http://minyounghuh.com/">Minyoung (Jacob) Huh</a>, MIT<br>
            <a href="https://stamarot.webgr.technion.ac.il/">Tamar Rott Shaham</a>, Technion (Fellowship winner, 2020)<br>
            <a href="https://payeah.net">Peiye Zhuang</a>, UIUC<br>
            <a href="http://people.csail.mit.edu/smirnov/">Dima Smirnov</a>, MIT<br>
            <a href="https://www.cs.tau.ac.il/~noafish/">Noa Fish</a>, Tel Aviv<br>
            <br>

            <dt><b>Masters/Undergrad [interns]</b></dt>
            <a href="http://people.csail.mit.edu/stevenliu/">Steven Liu</a>, MIT<br>
            <a href="https://sjooyoo.github.io/">Seungjoo Yoo</a>, Korea Univ (WIT scholarship winner, 2019)<br>
            <br>

            <dt><b>PhD/MS [university collaborators]</b></dt>
            <a href="https://stephanie-fu.github.io/">Stephanie Fu</a>, MIT<br>
            <a href="https://ssundaram21.github.io/">Shobhita Sundaram</a>, MIT<br>
            <a href="https://netanel-tamir.github.io/">Netanel Y. Tamir</a>, Weizmann<br>
            <a href="https://www.cs.princeton.edu/~pmanocha/">Pranay Manocha</a>, Princeton<br>
            <a href="https://rawanmg.github.io/">Rawan Alghofaili</a>, George Mason<br>
            <a href="http://alvinwan.com/">Alvin Wan</a>, UC Berkeley<br>
            <br>

            <!-- <dt><b>Undergrad [university collaborators]</b></dt> -->
            <!-- <a href="http://peterwang512.github.io/">Sheng-Yu Wang</a>, UC Berkeley <br> -->
        </dl>

        </span>

        <!-- <h3>@Berkeley</h3> -->
        <!-- <br> -->
         <span style="font-size: 14pt;">
            <b>@Berkeley</b>
        </span>

        <span style="font-size: 10pt;">
        <dl class="dl-horizontal">
          <dt><b>Undergraduates</b></dt>
            <a href="https://www.linkedin.com/in/xin-qin-4a83b9158/">Xin Qin</a>, next @ USC <br>
            <a href="https://www.linkedin.com/in/hemangjangle/">Hemang Jangle</a> <br>
            <a href="http://www.cs.utexas.edu/~alin/">Angela S. Lin</a>, next @ UT Austin <br>
            <a href="http://young-geng.xyz/">Xinyang Geng</a>, next @ UC Berkeley<br>
            <a href="https://tianheyu927.github.io/">Tianhe Yu</a>, next @ Stanford<br>
            <a href="https://www.linkedin.com/in/candrastefan/">Stefan A. Candra</a>
        </dl>

        <!-- </p> -->
        </span>
        <!-- </p><hr size="2" align="left" noshade=""> -->


        <h2>Teaching </h2>
        <!-- <a href="https://research.adobe.com/fellowship/">Adobe Research Fellowship</a> 2017 -->
        <span style="font-size: 12pt;">
        <b>Introduction to Artificial Intelligence (CS 188)</b>, UC Berkeley <br>
        <span style="font-size: 10pt;">
        Graduate Student Instructor (GSI) with Prof. <a href="http://people.eecs.berkeley.edu/~anca/">Anca Dragan</a> <br>
        Spring 2017<br>
        <br>
        <span style="font-size: 12pt;">
        <b>Computer Vision (CS 280)</b>, UC Berkeley <br>
        <span style="font-size: 10pt;">
        Graduate Student Instructor (GSI) with Prof. <a href="http://people.eecs.berkeley.edu/~efros/">Alexei A. Efros</a>, Prof. <a href="http://people.eecs.berkeley.edu/~trevor/">Trevor Darrell</a> <br>
        Spring 2016<br>
        <br>
        <span style="font-size: 12pt;">
        <b>Introduction to Circuits (ECE 2100)</b>, Cornell University <br>
        <span style="font-size: 10pt;">
        Teaching Assistant (TA) with Prof. <a href="https://molnargroup.ece.cornell.edu/">Alyosha Molnar</a> <br>
        Spring 2010<br>
        <br>

        <h2>My Name</h2>
        Confused by the contents of this page? Well, you may have been looking for Professor <a href="https://www.cs.sfu.ca/~haoz/" border="0">Richard Zhang</a> or Professor <a href="https://ryz.ece.illinois.edu/" border="0">Richard Zhang</a>. My Chinese name is ç« ç¿å˜‰.

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-75897335-1', 'auto');
  ga('send', 'pageview');
</script>
            
</font></td></tr></tbody></table><iframe frameborder="0" scrolling="no" style="border: 0px; display: none; background-color: transparent;"></iframe><div id="GOOGLE_INPUT_CHEXT_FLAG" input="null" input_stat="{&quot;tlang&quot;:true,&quot;tsbc&quot;:true,&quot;pun&quot;:true,&quot;mk&quot;:false,&quot;ss&quot;:true}" style="display: none;"></div></body></html>

